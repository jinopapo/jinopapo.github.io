<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[とてもつらい]]></title>
  <link href="http://jinopapo.github.io/atom.xml" rel="self"/>
  <link href="http://jinopapo.github.io/"/>
  <updated>2016-08-25T15:19:44+09:00</updated>
  <id>http://jinopapo.github.io/</id>
  <author>
    <name><![CDATA[Your Name]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Prml7]]></title>
    <link href="http://jinopapo.github.io/blog/2016/08/25/prml7/"/>
    <updated>2016-08-25T15:16:35+09:00</updated>
    <id>http://jinopapo.github.io/blog/2016/08/25/prml7</id>
    <content type="html"><![CDATA[<h1>疎な解を持つカーネルマシーン<a id="orgheadline67"></a></h1>

<p>カーネル法では、データ集合のサイズの2乗分の計算をしなければならなかったので、もっと疎な解を使ってカーネル関数を扱うことを考える。
ここでは、SVMとRVMついてみる。</p>

<h2>SVM<a id="orgheadline63"></a></h2>

<p>SVMでは、決定超平面と教師データとの最小の距離を持つベクトルをサポートベクターとしてサポートベクターとの距離マージンを最大するような決定超平面を決める。</p>

<h3>クラス分類<a id="orgheadline61"></a></h3>

<p>線形モデルを用いて、２クラス問題を解くことを考える。線形モデルは、
\[y(x)=w^{\mathrm{T}}\phi(x)+b\]
となる。教師データは、\(t\in\{-1.1\}\)となるデータを使い、符号により分類するとする。
決定面との距離は、
\[\frac{t_{n}y(x_{n})}{||w||}=\frac{t_{n}(w^{\mathrm{T}}\phi(x_{n})+b)}{||w||}\]
となる。ここで求めたいのは、マージンを最大する\(w\)と\(b\)なので、
\[\mathrm{argmax}_{w,b}\{\frac{1}{||w||}\min_{n}[t_{n}(w^{\mathrm{T}}\phi(x_{n})+b)]\}\]
を解ければよい。ここで、パラメータを定数倍しても解は変わらないので、適切に定数倍すると、サポートベクターについて、
\[t_{n}(w^{\mathrm{T}}\phi(x_{n})+b)=1\]
で表せる。これにより、成約式
\[t_{n}(w^{\mathrm{T}}\phi(x_{n})+b)\leq1\]
が成り立ち、求めるべきは、
\[\mathrm{argmin}_{w,b}\frac{1}{2}||w||^{2}\]
を制約式のもとでとくことになる。ここで、訓練集合にもノイズが含まれていることを考える。正しく分類された場合は0、誤って分類された場合は、\(|t_{n}-y(x_{n})|\)を表すスラック変数\(\xi\)を導入する。
スラック変数が、1以下の時には、正しく分類されていると考えることで、多少の誤差も許容する用になる。これにより
\[C\sum_{n=1}^{N}\xi_{n}+\frac{1}{2}||w||^{2}\]
を最小にするパラメータを決める問題になる。また制約式は、
\[t_{n}y(x_{n})\leq1-\xi_{n}\]
となる。Cは正則化係数と似たような働きをする。この最小化問題を解くためにラグランジェ未定乗数をもいてラグランジェ関数を定義すると、
\[L(w,b,\xi,a,\mu)=\frac{1}{2}||w||^{2}+C\sum_{n=1}^{N}\xi_{n}-\sum_{n=1}^{N}a_{n}\{t_{n}y(x_{n})-1+\xi_{n}\}-\sum_{n=1}^{N}\mu_{n}\xi_{n}\]
となる。\(a_{n}\)と\(\mu_{n}\)はラグランジェ乗数である。対応するKKT条件は、
\[a_{n}\leq0\]
\[t_{n}y(x_{n})-1+\xi_{n}\leq0\]
\[a_{n}(t_{n}y(x_{n})-1+\xi_{n})\]
\[\mu_{n}\leq0\]
\[\xi_{n}\leq0\]
\[\mu_{n}\xi_{n}\leq0\]
となる。ここで、各パラメータにおける極値をもとめると、
\[\frac{\partial L}{\partial w}=0\Rightarrow w=\sum_{n=1}^{N}a_{n}t_{n}\phi_(x_{n})\]
\[\frac{\partial L}{\partial b}=0\Rightarrow \sum_{n=1}^{N}a_{n}t_{n}=0\]
\[\frac{\partial L}{\partial \xi_{n}}=0\Rightarrow a_{n}=C-\mu_{n}\]
となる。これをラグランジェ関数に代入すると、
\[\hat{L}(a)=\sum_{n=1}^{N}a_{n}-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}a_{n}a_{m}t_{n}t_{m}k(x_{n},x_{m})\]
の双対表現で表わせる。ここでの制約式は、
\[0\leq a_{n}\leq C\]
\[\sum_{n=1}^{N}a_{n}t_{n}=0\]
で表され、この制約式のもと最小化するaを求めればよい。
この時求められるaは、サポートベクター以外は、\(a_{n}=0\)となるため、疎な解になる。
また、aはwの双対表現なので、bはについて考えると、bは、
\[t_{n}(\sum_{m\in S})a_{m}t_{m}k(x_{n},x_{m}+b)=1\]
で求められる。Sはサポートベクターの集合を表す。新しい入力に対する予測は、
\[y(x)=\sum_{n\in S}a_{n}t_{n}k(x,x_{n})+b\]
で表される。y
ここでは、計算機上の誤差を少なくするために、サポートベクターの平均をとり、
\[b=\frac{1}{N_{M}}\sum_{n\in M}(t_{n}-\sum_{m\in S}a_{m}t_{m}k(x_{n},x_{m}))\]
で求める。ここで、ラグランジェ双対表現されたラグランジェ関数の解き方について考える。この時、関数は2次式で凸関数なので、極値は最大値になる。
最もよく使われるのは、保護共役勾配法または、SMOと呼ばれるものである
SMOでは、二つのラグランジェ乗数を含む部分問題を逐次解いていき解を求める。
多クラスへの拡張を考える。多クラスでのSVMの適用はまだ解決していない。
提案されているものとして、あるクラスに分類されるとき、対応したSVMの教師には1を、他のSVMには、\(-\frac{1}{(K-1)}\)を用いる方法などがある。</p>

<h3>回帰<a id="orgheadline62"></a></h3>

<p>SVMで回帰することを考える。ここでは、決定超平面が回帰関数になる。
疎な解を求めるために、\(\epsilon\)許容誤差関数を用いて、誤差関数を定義すると、
\[C\sum_{n=1}^{N}E_{\epsilon}(y(x_{n})-t_{n})+\frac{1}{2}||w||^{2}\]
\[E_{\epsilon}(y(x)-t)=|y(x)-t|-\epsilon(|y(x)-t|-\epsilon\leq0) or 0\]
となる。この最適化問題をsvmでとく。
svmでは、超平面の外側であればどれだけ離れてもいてよかったが、回帰では、超平面に常に地下なければならないので、二つのスラック変数使う。
許容誤差で表される領域においては、両方0となる。許容誤差の範囲を出た時に上辺、下辺に対応したスラック変数の値が変化する。
これによりsvmの誤差関数は、
\[C\sum_{n=1}^{N}(\xi_{n}+\hat{\xi_{n}})+\frac{1}{2}||w||^{2}\]
となる。制約条件は、
\[t_{n}\leq y(x_{n})+\epsilon+\xi_{n}\]
\[t_{n}\geq y(x_{n})-\epsilon-\xi_{n}\]
となる。スラック変数は、非ゼロである。これをラグランジェ未定乗数で解く。
ラグランジェ関数は、
\[L=C\sum_{n=1}^{N}(\xi_{n}+\hat{\xi_{n}})-\sum_{n=1}^{N}(\mu_{n}\xi_{n}+\hat{\mu_{n}}\hat{\xi_{n}})-\sum_{n=1}^{N}a_{n}(\epsilon+\xi_{n}+y_{n}-t_{n})-\sum_{n=\hat{1}^{N}a_{n}}(\epsilon+\hat{\xi_{n}}-y_{n}+t_{n})]
となる。各パラメータにおいて極値を計算し、式変形すると、
\[\hat{L(a,\hat{a})}=-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}(a_{n}-\hat{a_{n}})(a_{m}-\hat{a_{m}})k(x_{n},x_{m})-\epsilon\sum_{n=1}^{N}(a_{n}-\hat{a_{n}})+\sum_{n=1}^{N}(a_{n}-\hat{a_{n}})t_{n}\]
で表わせ、\(a_{n}\)、\(\hat{a_{n}}\)について最大化する問題になる。KKT条件は、
\[a_{n(\epsilon+\xi_{n}+y_{n}-t_{n})=0}\]
\[a_{n(\epsilon+\hat{\xi_{n}}-y_{n}+t_{n})=0}\]
\[(C-a_{n})\xi_{n}=0\]
\[(C-\hat{a_{n}})\hat{\xi_{n}}=0\]
となる。bについては、
\[b=t_{n}-\epsilon-\sum_{n=1}^{N}(a_{n}-\hat{a_{n}})k(x_{n},x_{m})\]
となる。新しい予測は、
\[y(x)=\sum_{n\in S}^{N}(a_{n}-\hat{a_{n}})k(x,x_{n})+b\]
で表される。</p>

<h2>RVM<a id="orgheadline66"></a></h2>

<p>svmのような識別関数でなく、確率が出てくる識別器。svmのベイズ版みたいな感じ。</p>

<h3>回帰<a id="orgheadline64"></a></h3>

<p>基本的には、線形回帰と同じで、
目的変数tの条件付き確率は、
\[p(t|x,w,\beta)=N(t|y(X),\beta^{-1})\]
\[y(x)=w^{\mathrm{T}}\phi(x)\]
である。
データ集合が与えられた時の尤度関数は、
\[p(\mathbf{t}|X,w,\beta)=\prod_{n=1}^{N}p(t_{n}|x_{n},w,\beta)\]
となる。rvmでは、事前分布にパラメータごとに異なったハイパーパラメータを使うことで疎な解を得る。
事前分布は、
\[p(w|\alpha)=\prod_{i=1}^{M}N(w_{i}|0,\alpha_{i}^{-1})\]
となる。ベイズより、パラメータの事後分布は、
\[p(w|\mathbf{t},X,\alpha,\beta)=N(w,m,\Sigma)\]
\[m=\beta\Sigma\Phi^{\mathrm{T}}\mathbf{t}\]
\[\Sigma=(A+\beta\Phi^{\mathrm{T}}\Phi)^{-1}\]
となる。Aは、\(\alpha\)を対角成分に持つ対角行列、特徴空間を使わずカーネルを使う場合は、計画行列ではなくグラム行列になる。
最適化アルゴリズムをもちいてハイパーパラメータを最適化すると、予測分布は、
\[p(t|x,X,\alpha^{*},\beta^{*})=\int p(t|x,w,\beta^{*})p(w|X,\mathbf{t},\alpha^{*},\beta^{*})=N(t,m^{\mathrm{T}}\phi(x),\sigma^{2}(x))\]
\[m=\beta^{*}\Sigma\Phi^{\mathrm{T}}\mathbf{t}\]
\[\Sigma=(A^{*}+\beta^{*}\Phi^{\mathrm{T}}\Phi)^{-1}\]
\[\sigma^{2}(x)=(\beta)^{-1}-\phi(x)^{\mathrm{T}}\Sigma\phi(x)\]
となる。この時、多くの\(\alpha\)は、無限大になり、平均0に尖った形になるため基底関数を無効にする。残った基底関数に対応する入力を、関連ベクトルという。これは、サポートベクターに相当する。
ハイパーパラメータを最適化する方法を考える。一つは、エビデンス近似で、尤度関数\(p(\mathrm{t}|\alpha,\beta)\)を最大にする方法で、これは、導関数を0とおくか、EMアルゴリズムで求められる。
もっと高速に求められるものとして、逐次的疎ベイジアン学習アルゴリズムがある。
まずはじめに、適当に\(beta\)を初期化する。次に、基底関数を一つ選び、\(\alpha\)を決める。
\(\alpha\)は
\[a_{i}=\frac{s_{i}^{2}}{q_{i}^{2}-s_{i}}\]
\[s_{i}=\phi_{i}^{\mathrm{T}}C_{-i}^{-1}\phi_{i}\]
\[q_{i}=\phi_{i}^{\mathrm{T}}C_{-i}^{-1}\mathrm{t}\]
\[C=\beta^{-1}I+\Phi A^{-1}\Phi^{\mathrm{T}}\]
により決まる。\(C_{^i}\)は、Cから\(\alpha_{i}\)に関する項以外を除いたもの。
対応するハイパーパラメータ以外の初期値は無限とする。これにより今のモデルは、選ばれた一つの基底関数のみで初期化されている。
これを元に、\(\Sigma\)とｍを決める。
次に有効な基底関数を決める。
\(q_{i}^{2}>s_{i}\)ならば有効な基底関数として、\(\alpha_{i}\)を更新する。
\(q_{i}^{2}\geq s_{i}\)ならば無効な基底関数として、\(\alpha_{i}\)を無限にする。
再び\(beta、\Sigma、m\)を更新し、収束するまで繰り返す。</p>

<h3>分類<a id="orgheadline65"></a></h3>

<p>RVMでも出力に関してシグモイド関数を通すことで、\(t\in\{0,1\}\)を出力するようにする。
ここでは、解析的に積分ができないので、ラプラス近似を用いて計算する。また、ラベルに間違いはないと仮定しているので、\(\beta\)は存在しない。
まず、\(\alpha\)を初期化する。次に、今の\(\alpha\)に関して、ラプラス近似を行い、近似した分布で周辺尤度を計算する。最後に、尤度関数を最大にするように\(\alpha\)を更新する。
これらを収束するまで繰り返す。\(\alpha\)の最適化は回帰の時と同じものが使える。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prml6章]]></title>
    <link href="http://jinopapo.github.io/blog/2016/08/24/prml6/"/>
    <updated>2016-08-24T22:18:13+09:00</updated>
    <id>http://jinopapo.github.io/blog/2016/08/24/prml6</id>
    <content type="html"><![CDATA[<h1>カーネル法<a id="orgheadline60"></a></h1>

<p>データを特徴量空間に写像し、特徴量空間での内積によって新しい表現にとして利用する方法。非線形変換を行えるので、表現力が高い。
入力を特徴量空間に写像する関数を\(\phi(x)\)とすると、
\[k(x,x')=\phi(x)^{\mathrm{T}}\phi(x')\]
で表される関数をカーネル関数という。このカーネル関数を使うことで、カーネルトリックまたはカーネル置換を用いて、アルゴリズムを拡張できる。
また、カーネル関数では、基底関数を意識せずに計算できるので、無限の基底関数で表される空間なども扱える。
カーネル関数である条件は、要素がカーネル関数で表されるグラム行列が半正定値であること。
あるモデルを考えたときにパラメータwに関する関数を新しいパラメータaで置き換えることを双対表現という。双対表現を用いることで、カーネル関数で式を表現できる用になる。
カーネル関数には、ガウスカーネル関数など様々な物がある。</p>

<h2>RBFネットワーク<a id="orgheadline55"></a></h2>

<p>基底関数がある中心\(\mu\)との距離によって決まるものをRBFまたは、動径基底関数という。
RBFは、関数の補完や、入力にノイズが含まれる場合の補完問題などに使われる。RBFの中心は、ランダムに選んだり、k-meansみたいに直行最小二乗法を使ったりして、決める。</p>

<h3>Nadaraya-Watsonモデル<a id="orgheadline54"></a></h3>

<p>教師データが与えられた時の同時確率分布\(p(t,x)\)をParzen推定で求めるとすると、
\[p(x,t)=\frac{1}{N}\sum_{n=1}^{N}f(x-x_{n},t-t_{n})\]
となる。\(f(x,t)\)は、密度関数要素である。この時回帰関数\(y(x)\)を得るには、条件付き期待値を求めればいいので、
\[y(x)=E[t|x]=\int_{-\infty}^{\infty}tp(t|x)dt\]
\[=\frac{\sum_{n}\int tf(x-x_{n},t-t_{n})dt}{\sum_{m}\int tf(x-x_{m},t-t_{m})dt}\]
となる。ここで、密度関数の平均が0だとすると、
\[y(x)=\sum_{n}k(x,x_{n})t_{n}\]
\[k(x,x_{n})=\frac{g(x-x_{n})}{\sum_{m}g(x-x_{m})}\]
\[g(x)=\int_{-\infty}^{\infty}f(x,t)dt\]
となる。これは、Nadaraya-Watsonモデルまたはカーネル回帰と言われる。</p>

<h2>ガウス過程<a id="orgheadline59"></a></h2>

<p>入力\(x\)が与えられた時の出力\(y(x)\)の同時確率分布が正規分布であるとしたとき、ガウス過程という。特に入力ベクトルが2次元の時にはガウス確率場といわれる。
ガウス過程のいいところは、同時確率分布が、平均と分散の二つで表現できること、実際は、平均は0として扱われることがおおい。分散には、カーネル関数が用いられる。</p>

<h3>ガウス過程による回帰<a id="orgheadline56"></a></h3>

<p>まず目的変数の発生確率について考える。出力yが与えられた時の目的変数tの同時確率分布は、
\[p(t|y)=N(t|y,\beta^{-1}I)\]
となる。ガウス過程の定理より周辺分布\(p(y)\)は、
\[p(y)=N(y|0,K)\]
となる。Kを決めるカーネル関数には、似ている入力に対応する出力の相関が大きくなるようなものを選ぶ。以上より周辺分布\(p(t)\)は、
\[p(t)=\int p(t|y)p(y)dy=N(t|0,K)\]
\[C(x_{n},x_{m})=k(x_{n},x_{m})+\beta^{-1}\delta_{nm}\]
となる。この時のカーネル関数には、
\[k(x_{n},x_{m})=\theta_{0}\exp\{-\frac{\theta_{1}}{2}||x_{n}-x_{m}||\}+\theta_{2}+\theta_{3}x_{n}^{\mathrm{T}}x_{m}\]
のような形のものがよく使われる。
次に、教師データとして、\(\mathbf{t}=(t_{1},&hellip;,t_{n})\)が与えられた時の新しい\(t_{n+1}\)の条件付き確率\(p(t_{n+1}|\mathbf{t})\)について考える。
分割された正規分布として捉えるために、分割前の正規分布\(p(\mathbf{t}_{n+1})\)について考えると、
\[p(\mathbf{t}_{n+1})=N(\mathbf{t}_{n+1}|0,C_{n+1})\]
となる。ここで共分散行列を分解すると、
\[
  C_{n+1} = \left(</p>

<p>\begin{array}{cc}
  C_{n} &amp; k \
  k^{\mathrm{T}} &amp; c
\end{array}</p>

<p>  \right)
\]
\[c=k(x_{N+1},x_{N+1})+\beta^{-1}\]
となる。kは、\(k(x_{n},x_{N+1})\)を要素にもつベクトル。よって、条件付き確率の平均と分散は、
\[m(x_{N+1})=k^{\mathrm{T}}C_{N}^{-1}t\]
\[\sigma^{2}(x_{N+1})=c-k^{\mathrm{T}}C_{N}^{-1}k\]
となる。ここで、ハイパーパラメータの学習について考える。ハイパーパラメータを\(\theta\)とすると、対数尤度は、
\[\ln p(t|\theta)=-\frac{1}{2}\ln|C_{n}|-\frac{1}{2}t^{\mathrm{T}}C_{N}^{-1}t-\frac{N}{2}\ln(2\pi)\]
となる。非線形なので、微分を求めると、
\[\frac{\partial}{\partial\theta_{i}}\ln p(t|\theta)=-\frac{1}{2}\mathrm{Tr}(C_{N}^{-1})\frac{\partial C_{N}}{\partial\theta_{i}}+\frac{1}{2}t^{\mathrm{T}}C_{N}^{-1}\frac{\partial C_{N}}{\partial\theta_{i}}C_{N}^{-1}t\]
となる。あとは、共役勾配法などで求められる。このように、最尤推定でハイパーパラメータを決めるのを関連度自動決定またはARDといわれ、入力間の相対的な重要度をデータから決めることになる。
ARDをカーネル関数に組み込んだものが、上に示した回帰問題によく使われる関数と一致する。</p>

<h3>ガウス過程による分類<a id="orgheadline58"></a></h3>

<p>ガウス過程による出力は、実数全体の値を取るのでガウス過程の出力aをシグモイド関数に通して、確率値に収めることで分類問題に使う。
2クラスの分類について考える。目標変数の確率分布は、
\[p(t|a)=\sigma(a)^{t}(1-\sigma(a))^{1-t}\]
となる。ここで、N個の教師データが与えられた時について考える。
新たな入力に関するガウス過程による事前分布は、
\[p(a_{N+1})=N(a_{N+1}|0,C_{N+1})\]
となる。分類問題では、教師データは必ず正しいとして使うので、ノイズ項は含まないが、安定性の問題からノイズ項に相当するパラメータ\(\nu\)で表される項を入れるとよい。
よって共分散行列は、
\[C(x_{n},x_{m})=k(x_{n},x_{m})+\nu\delta_{nm}\]
となる。2クラス分類なので、片方がわかればいいので、\(t=1\)の場合について考えると、予測分布は、
\[p(t_{N+f}=1|t_{N})=\int p(t_{N+1}|a_{N+1})p(a_{N+1}|t_{n})da_{N+1}\]
\[p(t_{N+1}|a_{N+1})=\sigma(a_{N+1})\]
となる。この積分をガウス分布に近似することで解く。
ガウス分布に近似するには、線分推論法や、EP法、ラプラス近似などがある。ここでは、ラプラス近似のみに触れる。</p>

<ol>
<li><p>ラプラス近似による分類</p>

<p>事後分布\(p(a_{N+1}|t_{N})\)について考えると、\(p(t_{N}|a_{N+1},a_{N})=(p(t_{N}|a_{N})\)より
\[p(a_{N+1}|t_{N})=\int p(a_{N+1},a_{N}|t_{N})da_{N}=\int p(a_{N+1}|a_{N})p(a_{N}|t_{N})da_{N}\]
となる。条件付き確率\(p(a_{N+1}|a_{N})\)はガウス過程より、
\[p(a_{N+1}|a_{N})=N(a_{N+1}|k^{\mathrm{T}}C_{N}^{-1}a_{N},c-k^{\mathrm{T}}C_{N}^{-1}k)\]
となる。事前分布は共分散が\(C_{N}\)の正規分布であたえられ、データに関する項は、
\[p(t_{N}|a_{N})=\prod_{n=1}^{N}e^{a_{n}t_{n}}\sigma(-a_{n})\]
で与えられる。\(p(a_{N+1}|a_{N})\)をテイラー展開すると、
\[\Psi(a_{N})=-\frac{1}{2}a_{N}^{\mathrm{T}}C_{N}^{-1}a_{N}-\frac{N}{2}\ln(2\pi)-\frac{1}{2}\ln|C_{N}|+t_{N}^{\mathrm{T}}a_{N}-\sum_{n=1}^{N}\ln(1+e^{a_{n}})\]
となる。この時のモードは、非線形なので、反復再重み付け最小二乗法で求める。また、ヘッセ行列は
\[H=W_{N}+C_{N}^{-1}\]
となる。\(W_{N}\)は\(\sigma(a_{N})(1-\sigma(a_{N}))\)を要素に持つ対角行列である。
ヘッセ行列も\(a_{N}\)に関する式なので、ニュートン-ラフソン法で繰り返し更新して求める。
更新式は、
\[a_{N}^{NEW}=C_{N}(I+W_{N}C_{N})^{-1}\{t_{N}-\sigma_{N}+W_{N}a_{N}\}\]
となる。ラプラス近似で求められた事後分布は、
\[q(a_{N})=N(a_{N}|a^{*}_{N},H^{-1})\]
\[a_{N}^{*}=C_{N}(t_{N}-\sigma_{N})\]
となる。よって予測分布の平均と分散は、
\[E[a_{N+1}|t_{N}]=k^{\mathrm{T}}(t_{N}-\sigma_{N})\]
\[var[a_{N+1}|t_{N}]=c-k^{\mathrm{T}}(W_{N}^{-1}+C_{N}^{-1})^{-1}k\]
となる。まだ、カーネル関数のハイパーパラメータの問題が残っている。
尤度関数\(p(t_{n}|\theta)\)を最大にすることでハイパーパラメータを決める。尤度関数は、
\[p(t_{N}|\theta)=\int p(t_{N}|a_{N})p(a_{N}|\theta)da_{N}\]
となる。これもラプラス近似によって解くことができる。多クラスに拡張するときは、シグモイド関数の代わりにソフトマックス関数を用いるとよい。</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prml5章]]></title>
    <link href="http://jinopapo.github.io/blog/2016/08/22/prml5/"/>
    <updated>2016-08-22T15:21:09+09:00</updated>
    <id>http://jinopapo.github.io/blog/2016/08/22/prml5</id>
    <content type="html"><![CDATA[<h1>ニューラルネットワーク<a id="orgheadline53"></a></h1>

<p>今まで自分たちで基底関数を考えてきたが、基底関数の数だけ決めて、あとは、全部学習で決める。</p>

<h2>フィードフォワードネットワーク関数<a id="orgheadline45"></a></h2>

<p>基本的なニューラルネットワークにおけるモデルは、
\[z_{j}=h(a_{j})\]
\[a_{j}=\sum_{i=1}^{D}w_{ji}x_{i}+w_{j0}\]
となる。\(D\)は入力次元数、\(w\)は重みパラメータ、\(w_{0}\)はバイアスを表す。\(h()\)は活性化関数と言われ、シグモイド関数がおもに用いられる。
ニューラルネットワークでは、\(z_{k}\)をユニットといい、ユニットをつなげて、ネットワークを作り、それに応じて出力する。ユニットに入力を与えて出力を得るのをフォワードプロパゲーションという。
出力値として使われるユニット以外を隠れユニットという。
またニューラルネットワークは、ユニット同士の入力結合のパラメータと出力結合のパラメータを入れ替えたり、パラメータの一部の符号を反転させても同じ出力が得られるので、同じ関数で自由度がユニット数を\(M\)とすると\(M!2^{M}\)ある。</p>

<h2>学習<a id="orgheadline46"></a></h2>

<p>ニューラルネットワークでは、誤差関数を最小にすることで尤度関数を最大にすることと同じ結果を得ている。
誤差を最小にするのは、誤差関数で表される曲面で極小値を見つけることになる。ニューラルネットワークでは、バックプロパゲーションで勾配情報を計算し、勾配情報を元に確率的勾配硬化法を使い、極小値を求める。
バックプロパゲーションについて考える。
まず始めにフォワードプロパゲーションを行いユニットの入力値\(a_{j}\)と、ユニットの値\(z_{j}\)を求める。
次に、誤差関数の微分を考える、
誤差関数の微分は、
\[\frac{\partial E_{n}}{\partial w_{ji}}=\frac{\partial E_{n}}{\partial a_{j}}\frac{\partial a_{j}}{\partial w_{ji}}\]
となる。ここで、
\[\frac{\partial a_{j}}{\partial w_{ji}}=z_{i}\]
となる。ここで、
\[\frac{\partial E_{n}}{\partial a_{j}}=\delta_{j}\]
とする。これは、誤差を呼ばれる。
出力ユニットにおいて、活性化関数に正準連結関数を用いて場合誤差は、
\[\delta_{k}=y_{k}-t_{k}\]
となる。また、隠れユニットの誤差は、出力ユニットの誤差を利用して、
\[\delta_{j}=h'(a_{j})\sum_{k}w_{kj}\delta_{k}\]
となる。このように出力から逆に誤差を伝搬していき、伝搬された値をもとに、誤差関数の微分を求めることができる。
また、実装の際には、極小幅での中心差分での近似を用いて確認するのがよい。
また複数のネットワークの組み合わせの場合には、誤差関数の微分は、
\[\frac{\partial E}{\partial w}=\sum_{k,j}\frac{\partial E}{\partial y_{k}}\frac{\partial y_{k}}{\partial z_{j}}\frac{\partial z_{j}}{\partial w}\]
よなる、右辺の真ん中の項は、ヤコビ行列になる。
ヤコビ行列は、バックプロパゲーションと同じ用に、順伝搬して、出力ユニットの活性化関数に応じた誤差を伝搬させ、各要素を計算する。</p>

<h2>ヘッセ行列<a id="orgheadline47"></a></h2>

<p>一次微分のヤコビ行列だけでなく、二次微分のヘッセ行列\((W*W)\)も求めておくといいことがある。例えば、最適化アルゴリズムや、再学習アルゴリズムなどに使うことができる。
対角化で近似することを考える。
この時対角成分は、
\[\frac{\partial^{2} E_{n}}{\partial w_{ji}^{2}}=\frac{\partial^{2} E_{n}}{\partial a_{j}^{2}}z_{i}^{2}\]
となる。右辺の二次微分は、連鎖法則を再帰的に用いることで求められ、非対角項を無視して、
\[\frac{\partial^{2} E_{n}}{\partial a_{j}^{2}}=h'(a_{j})^{2}\sum_{k}w_{kj}^{2}\frac{\partial^{2} E_{n}}{\partial a_{k}^{2}}+h'&lsquo;(a_{j})\sum_{k}w_{kj}\frac{\partial E_{n}}{\partial a_{j}}\]
となる。実際のヘッセ行列は、非対角なので、これだとうまく近似できない場合が多い。
そこで、外積による近似を考える。誤差関数に、二乗誤差を用いているときを考えると、ヘッセ行列は、
\[H=\sum_{n=1}^{N}\nabla y_{n}(\nabla y_{n})^{\mathrm{T}}+\sum_{n=1}^{N}(y_{n}-t_{n})\nabla\nabla y_{n}\]
となる。ここで、第二項は非常に小さく無視できるので、
\[H\simeq\sum_{n=1}^{N}b_{n}b_{n}^{\mathrm{T}}\]
\[b_{n}=\nabla a_{n}\]
で近似できる。これを外積による近似またはLevenberg-Marquardt近似という。
他の誤差関数でも、同じように二次微分の二項を無視した形で近似できる。
外積による近似を用いて、逆行列を効率的に求めることを考える。
ヘッセ行列を逐次的に求めるとすると、
\[H_{L+1}=H_{L}+b_{L+1}b_{L+1}^{\mathrm{T}}\]
となる。ここで、Woodburyの公式を使うと、逆行列は、
\[H_{L+1}^{-1}=H_{L}^{-1}-\frac{H_{L}^{-1}b_{L+1}b_{L+1}^{\mathrm{T}}H_{L}^{-1}}{1+b_{L+1}^{\mathrm{T}}H_{L}^{-1}b_{L+1}}\]
で更新できる。
またヘッセ行列は、ほとんどベクトルとの積で用いられる。そこで、直接ベクトルとの積を求めることを考える。ベクトルとの積は、
\[v^{\mathrm{T}}H=v^{\mathrm{T}}\nabla(\nabla E)=R\{\nabla E\}\]
で表される。これより、計算するときに、
\[R{w}=v\]
として同じように計算するといいことがわかる。</p>

<h2>正則化<a id="orgheadline50"></a></h2>

<p>ニューラルネットワークにおける正則化について考える。
今まで通りの荷重減衰の正則化項では、無矛盾性を持っていないのでうまくいかない。
無矛盾性とは、入力または、教師データを線形変換させたときに、線形変換分重みだけがことなるモデルが作れること。
無矛盾性を持っていないと、線形変換した同じベクトルを別のクラスに分類したりしてしまう。
ここで、簡単なものとして、変速事前分布を仮定して、無矛盾性をもつ正則化項を表すと
\[\sum_{k}\frac{\lambda_{k}}{2}\sum_{w\in W_{k}}w^{2}\]
となる。また事前分布は、
\[p(w)\propto\exp(\frac{1}{2}\sum_{k}\alpha_{k}||w||^{2}_{k})\]
となる。
また別の正則化の方法として、学習が進むと有効パラメータが増えることを利用して、学習用データの誤差が最小のとこではなく、テストデータの値が最小になるようなとこで学習をやめる方法がある。
正則化を加えることで、不変性を得ることがある。不変性を持っていると、同じ画像を線形変換した物でも同じクラスに分類したりすることができる。
方法として、</p>

<ul>
<li>教師データに線形変換したものをつくrい教師データの量を増やす。</li>
<li>正則化項を加えることで、入力に対して出力が変化したらペナルティを課すようにする。接線伝播法といわれる。</li>
<li>入力に線形変換で不変となるような特徴量を使う。</li>
<li>ニューラルネットワークに不変性を担保するような構造を組み込む。例えば、CNNとか局所的な情報など。</li>
</ul>


<p>などがある。</p>

<h3>接線伝播法<a id="orgheadline48"></a></h3>

<p>入力を変換してできる新しい入力集合をMとすると、変換の影響は、局所的には接ベクトルで近似できることを利用し誤差関数を抑える方法。
変換がパラメータ\(\xi\)で決まるとして、入力を変換したものを\(s(x_{n,\xi})\)とすると、接ベクトルは、
\[\tau_{n}=\frac{\partial s(x_{n,\xi})}{\partial \xi}|_{\xi=0}\]
となる。出力を\(\xi\)で微分すると、
\[\frac{\partial y_{k}}{\partial \xi}|_{\xi=0}=\sum_{i=1}^{D}J_{ki}\tau_{i}\]
となる。これを利用すると誤差関数は、
\[\hat{E}=E+\lambda\Omega\]
\[\Omega=\frac{1}{2}\sum_{n}\sum_{k}(\sum_{i=1}^{D}J_{ki}\tau_{i})^{2}\]
となる。こうすることで、入力と出力の変化に応じたペナルティが付き、不変性を得ることができる。</p>

<h3>ソフト重み共有<a id="orgheadline49"></a></h3>

<p>あるグループに属する重みはすべて同じ重みを使うような正則化項を入れることで、複雑さを抑える。
単純な荷重減衰では、事前分布に正規分布を選んでいたのですべての重みを同じように扱ったが、ここでは、事前分布に混合ガウス分布を使うことで、複数グループに分けて扱う。
誤差関数は、
\[\hat{E}(w)=E(w)+\Omega(w)\]
\[\Omega(w)=-\sum_{i}\ln(\sum_{j=1}^{M}\pi_{j}N(w_{i}|\mu_{j},sigma_{i}^{2}))\]
となる。これを最小にするには、共役勾配法や、準ニュートン法などを使う。</p>

<h2>混合密度ネットワーク<a id="orgheadline51"></a></h2>

<p>答えが複数あるような問題いおいては、直接ニューラルネットワークで扱うのは向いていない。
こういう問題では、多峰性をもっているため、ニューラルネットワークだと変なところに収束する。
そこで、事後分布\(p(t|x)=\sum_{k=1}^{K}\pi_{k}N(t|\mu_{k},\sigma^{2}_{k}I)\)パラメータを出力するネットワークを考える。
混合係数の出力ユニットにはソフトマックス関数、分散の出力ユニットにはexp関数、平均の出力ユニットには、線形関数を使うとよい。
誤差関数には、
\[E(w)=-\sum_{n=1}^{N}\ln\{\sum_{k=1}^{K}\pi_{k}(x{n},w)N(t_{n}|\mu_{k}(x_{n},w),\sigma_{k}^{2}(x_{n},w)I)\}\]
を用いる。
また、各パラメータの微分は、
\[\frac{\partial E_{n}}{\partial a_{k}^{\pi}}=\pi_{k}-\gamma_{nk}\]
\[\frac{\partial E_{n}}{\partial a_{k}^{\mu}}=\gamma_{nk}\{\frac{\mu_{kl}-t_{kl}}{\sigma^{2}_{k}}\}\]
\[\frac{\partial E_{n}}{\partial a_{k}^{\sigma}}=\gamma_{nk}(L-\frac{||t_{n}-\mu_{k}||^{2}}{\sigma^{2}_{k}})\]
\[\gamma_{nk}(t_{n}|x_{n})=\frac{\pi_{k}N_{nk}}{\sum_{l=1}^{K}\pi_{l}N_{nl}}\]
となる。</p>

<h2>ベイズネットワーク<a id="orgheadline52"></a></h2>

<p>ニューラルネットワークをベイズ的に扱うことを考える。極大値をピークとする正規分布をラプラス近似し、局所的に見てパラメータに線形な形で近似する。
事後分布が正規分布であるとすると、
\[p(t|x,w,\beta)=N(t|y(x,w),\beta^{-1})\]
となる。この時の重みの事前分布は、
\[p(w|\alpha)=N(w|0,\alpha^{-1}I)\]
となる。ここで、教師データDが与えられた時の尤度関数は、
\[p(D|w,\beta)=\prod_{n=1}^{N}N(t_{n}|y(x_{n},w),\beta^{-1})\]
となり、事後分布は、
\[p(w|D,\alpha,\beta)\propto p(w|\alpha)p(D|w,\beta)\]
となる。ここで、事後分布をラプラス近似して、正規分布を求める。
\(\alpha\)、\(\beta\)が固定であるとすると、極大値は、誤差伝播で求められる。極大値をモードとしてラプラス近似すると正規分布は、
\[q(w|D)=N(w|w_{MAP},A^{-1})\]
\[A=-\alpha I+\beta H\]
となる。Hは、\(w_{MAP}\)の時のヘッセ行列。またニューラルネットワークの出力は非線形だが、ニューラルネットワークの出力の変化が、wの変化に比べて小さいとすると、\(w_{MAP}\)周りでテイラー級数展開でき、
\[y(x,w)\simeq y(x,w_{MAP})+g^{\mathrm{T}}(w-w_{MAP})\]
\[g=\nabla_{w}y(x,w)|_{w=w_{MAP}}\]
となり、線形ガウス分布で近似できた。よって予測分布は、
\[p(t|x,D,\alpha,\beta)=N(t|y(x,w_{MAP})|\sigma^{2}(x))\]
\[\sigma^{2}(x)=\beta^{-1}+g^{\mathrm{T}}A^{-1}g\]
となる。これまで、ハイパーパラメータを固定にしてきたが、ハイパーパラメータの求め方について考える。
ハイパーパラメータの周辺尤度は、
\[p(D|\alpha,\beta)=\int p(D|w,\beta)p(w|\alpha)dw\]
で表され、対数尤度を最大にするようなパラメータを求めればよい。
まず、\(\alpha\)について最大化する。固有方程式を、
\[\beta Hu_{i}=\lambda_{i}u_{i}\]
とすると、
\[\alpha=\frac{\gamma}{w_{MAP}^{\mathrm{T}}w_{MAP}}\]
\[\gamma=\sum_{i=1}^{W}\frac{\lambda_{i}}{\alpha+\lambda_{i}}\]
となる。\(beta\)について最大化すると、
\[\frac{1}{\beta}=\frac{1}{N-\gamma}\sum_{n=1}^{N}\{y(x_{n},w_{MAP})-t_{n}\}^{2}\]
となる。ハイパーパラメータを更新することで、\(w_{MAP}\)が変わるので、事後分布とハイパーパラメータを交互に更新して求めることができる。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prml4]]></title>
    <link href="http://jinopapo.github.io/blog/2016/08/18/prml4/"/>
    <updated>2016-08-18T19:00:33+09:00</updated>
    <id>http://jinopapo.github.io/blog/2016/08/18/prml4</id>
    <content type="html"><![CDATA[<h1>線形識別モデル<a id="orgheadline44"></a></h1>

<p>入力ベクトル\(x\)を\(K\)個のクラスに分類することを分類問題という。
この時、入力空間は、決定領域といわれるクラスごとの領域に分割される。
決定領域の境界を決定境界という。
ここでは、D次元の入力空間をD-1次元の決定平面で分割する線形識別モデルについて考える。
またこの時
正しく分割できる驟雨号を線形決定可能な集合という。
回帰モデルの時違い、クラスは離散的な値になるので、出力は、非線形関数fを使って
\[y(x)=f(w^{\mathrm{T}}x+w_{0})\]
で表し、一般線形モデルといわれる。この時のfを活性化関数という。</p>

<h2>識別関数<a id="orgheadline38"></a></h2>

<p>最も簡単な線形識別関数は、
\[y(x)=w^{\mathrm{T}}x+w_{0}\]
で表され、\(w\)を重みベクトル、\(w_{0}\)をバイアスパラメータという。またバイアスパラメータのマイナスは、しきい値パラメータと呼ばれる。
このとき、決定面は、\(y(x)=0\)であたえられる。この時、wは決定面に直行し、y(x)は、xからの直行距離を表す。
他クラスに分類する場合、クラスに属すかいないかを決める関数を複数個組み合わせる1対他分類器や、
あるクラスか、また別のクラスに分類する関数を複数用いて、多数決によって分類する1対1分類器などがあるが、曖昧さがうまれてしまう。
そこで、K個の線形分類器からなるKクラス識別を考えると
\[y_{k}(x)=w_{k}^{\mathrm{T}}x+w_{k0}\]
で表される。
この時、最も大きい値のクラスに入力は分類される。この時の決定面は、
\[(w_{k}-w_{j})^{\mathrm{T}}x+(w_{k0}-w_{j0})=0\]
となる。またこの決定面は、一つに連結している。
パラメータの決め方には、最小二乗、フィッシャーの線形判別、パーセプトロンアルゴリズムなどがある。</p>

<h3>最小二乗<a id="orgheadline35"></a></h3>

<p>ここでは、目的変数tに1-fo-K符号化法を使う。
各線形識別器をひとまとめにすると
\[\mathbf{y}(x)=\mathbf{W}^{\mathrm{T}}\mathbf{x}\]
となる。\(\mathbf{W}\)は、要素が\((w_{k0},\mathbf{w}_{k}^{\mathrm{T}})\)で表され、
\(\mathbf{x}\)の要素は、\((1,\mathbf{x}^{\mathrm{T}})^{\mathrm{T}}\)で表される。
この時の二乗誤差は、
\[E_{D}(\mathbf{W})=\frac{1}{2}\mathrm{Tr}\{(\mathbf{XW}-\mathbf{T})^{\mathrm{T}}(\mathbf{XW}-\mathbf{T})\}\]
となる。また、\(\mathbf{X}\)の要素は、\(\mathbf{x}_{n}^{\mathrm{T}}\)、\(\mathbf{T}\)の要素は、\(t_{n}^{\mathrm{T}}\)となる。
二乗誤差の導関数を0として解くと、
\[\mathbf{W}=(\mathbf{X}^{\mathrm{T}}\mathbf{X})\mathbf{X}^{\mathrm{T}}\mathbf{T}\]
となる。
よって識別関数は、
\[y(x)=\mathbf{T}^{\mathrm{T}}\mathbf{X}^{\dag\mathrm{T}}\mathbf{x}\]
\[\mathbf{X}^{\dag}=(\mathbf{X}^{\mathrm{T}}\mathbf{X})\mathbf{X}^{\mathrm{T}}\]
となる。
最小二乗では、出力の和が1になってはいるが、正規化できていなかったり、ノイズに弱かったり、尤度関数に正規分布を仮定しているので、問題がある。</p>

<h3>フィッシャーの線形判別<a id="orgheadline36"></a></h3>

<p>高次元の入力の次元を削減し、低次元で分類する方法。
まず、2クラスの分類器について考える。
入力を1次元に射影すると、
\[y=w^{\mathrm{T}}x\]
で表せる。
各クラスの平均ベクトルは、
\[m_{k}=\frac{1}{N_{k}}\sum_{n\in C_{k}}x_{n}\]
となる。
クラス\(C_{k}\)から射影されたデータのクラス内分散は、
\[s_{k}^{2}=\sum_{n\in C_{k}}(y_{n}-m_{k})^{2}\]
となる。
フィッシャーの判断規準は、
\[J(w)=\frac{(m_{2}-m_{1})^{2}}{s_{1}^{2}+s_{2}^{2}}\]
\[J(w)=\frac{w^{\mathrm{T}}S_{B}w}{w^{\mathrm{T}}S_{W}w}\]
\[S_{B}=(m_{2}-m_{1})(m_{2}-m_{1})^{\mathrm{T}}\]
\[S_{W}=\sum{n\in C_{1}}(x_{n}-m_{1})(x_{n}-m_{1})^{\mathrm{T}}+\sum{n\in C_{2}}(x_{n}-m_{2})(x_{n}-m_{2})^{\mathrm{T}}\]
であらわされる。また、\(S_{B}\)をクラス間共分散、\(S_{W}\)を総クラス内共分散という。
導関数を0とおき、解くと
\[(w^{\mathrm{T}}S_{B}w)S_{W}w=(w^{\mathrm{T}}S_{W}w)S_{B}w\]
を満たすJ(w)が最大となるのがわかる。また、\(S_{B}\)は、\(m_{2}-m_{1}\)と同じ向きのベクトルで、()の中はスカラなので、
\[w\propto S_{w}^{-1}(m_{2}-m_{1})\]
となる。これは、フィッシャーの線形判別といわれる。
これにより射影された空間では、\(y(x)\geq y_{0}\)が\(C_{1}\)に分類され、それ以外が\(C_{2}\)に分類される。
多クラスの場合について考える。
分類クラスより、入力次元が大きいとする。
入力データを、k個の特徴に変換すると、
\[y=W^{\mathrm{T}}x\]
となる。Wは、\(w_{k}\)を列の要素としている。
クラス内分散を考えると、
\[S_{W}=\sum_{k=1}^{K}S_{k}\]
\[S_{k}=\sum_{n\in C_{k}}(x_{n}-m_{k})(x_{n}-m_{k})^{\mathrm{T}}\]
\[m_{k}=\frac{1}{N}\sum_{n\in C_{k}}x_{n}\]
となる。
次に、総分散行列を考えると、
\[S_{T}=\sum_{n=1}^{N}(x_{n}-m)(x_{n}-m)^{\mathrm{T}}\]
\[m=\frac{1}{N}\sum_{k=1}^{K}N_{k}m_{k}\]
となる。
また、総分散行列は、総クラス間分散とクラス内分散に分解できるので、
\[S_{T}=S_{W}+S_{B}\]
\[S_{B}=\sum_{k=1}^{K}N_{k}(m_{k}-m)(m_{k}-m)^{\mathrm{T}}\]
と洗わせる。これは、x空間でのものだが、射影した空間でも成り立つ。
ここで、クラス内共分散が小さくクラス間共分散が大きい時に大きくなるスカラを考える。
例の一つは、
\[J(W)=\mathrm{Tr}S_{W}^{-1}S_{B}\]
\[J(W)=\mathrm{Tr}\{(W^{\mathrm{T}}S_{W}W)^{-1}(W^{\mathrm{T}}S_{B}W)\}\]
で表される。また、この方法では、K個以上の特徴が発見できないことが知られている。</p>

<h3>パーセプトロンアルゴリズム<a id="orgheadline37"></a></h3>

<p>以下のような線形識別モデルを考える。
\[y(x)=f(w^\mathrm{T}\phi(x))\]
この時の活性化関数は、入力が0以上ならば1、以下なら-1を出力するステップ関数を使う。
目的変数は、\(C_{1}\)なら1、\(C_{2}\)なら-1という風に1と-1で表す。
ここで、パラメータを関数を特徴にかけた時、\(C_{1}\geq 0\)、\(C_{2}&lt;0\)となるようなパラメータを考える。
この時、誤差関数を考えると、
\[E_{P}(w)-\sum_{n\in M}w^{\mathrm{T}\phi(x_{n})t_{n}}\]
となり、これを最小にすることを考える。Mは誤認識された集合を表す。
この式は、wに関して線形なので、確率的勾配降下法を使って最小化できる。
更新式は、
\[w^{\tau}=w^{\tau-1}-\eta E_{P}\]
となる。誤差データのみを見ているので、更新によって新たに誤認識される場合もあるがこれは収束することがわかっている。</p>

<h2>確率的生成モデル<a id="orgheadline39"></a></h2>

<p>ベイズの定理を使って、条件付き確率\(p(x|C_{i})\)と事前分布\(p(C_{i})\)から事後分布\(p(C_{i}|x)\)を計算する。
事後確率の形について考える。
2クラスの場合、
\[p(C_{1}|x)=\frac{p(x|C_{1})p(C_{1})}{p(x|C_{1})p(C_{1})+p(x|C_{2})p(C_{2})}=\sigma(a)\]
\[a=\ln\frac{p(x|C_{1})p(C_{1})}{p(x|C_{2})p(C_{2})}\]
となる。\(\sigma(a)\)はロジスティクスシグモイド関数である。
多クラスの場合、
\[p(C_{k}|x)=\frac{p(x|C_{k})p(C_{k})}{\sum_{j}p(x|C_{j})p(C_{j})}\]
\[p(C_{k}|x)=\frac{\exp(a_{k})}{\sum_{j}\exp(a_{j})}\]
\[a_{k}=\ln(p(x|C_{k})p(C_{k}))\]
となる。この時の事後確率の関数は、ソフトマックス関数といわれる。
入力が連続値である時を考える。
すべてのクラスが同じ共分散行列を共有すると仮定し、条件付き分布は、正規分布だと仮定すると、
\[p(x|C_{k})=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^{\frac{1}{2}}}\exp\{-\frac{1}{2}(x-\mu_{k})^{\mathrm{T}}\Sigma^{-1}(x-\mu_{k})\}\]
となる。よって2クラスの時の、\(C_{1}\)の事後確率は、
\[p(C_{1}|x)=\sigma(w^{\mathrm{T}}x+w_{0})\]
\[w=\Sigma^{-1}(\mu_{1}-\mu_{2})\]
\[w_{0}=-\frac{1}{2}\mu_{1}^{\mathrm{T}}\Sigma^{-1}\mu_{1}+\frac{1}{2}\mu_{2}^{\mathrm{T}}\Sigma^{-1}\mu_{2}+\ln\frac{p(C_{1})}{p(C_{2})}\]
となる。
多クラスの時は、
\[a_{k}=w_{k}^{\mathrm{T}}x+w_{k0}\]
\[w_{k}=\Sigma^{-1}\mu_{k}\]
\[w_{k0}=-\frac{1}{2}\mu_{k}^{\mathrm{T}}\Sigma^{-1}\mu_{k}+\ln p(C_{k})\]
となる。
尤度関数について考える。
2クラスの時について考える。条件付き分布が正規分布であると仮定し、\(C_{1}\)の事前分布が\(\pi\)であるとすると、尤度関数は、
\[p(\mathbf{t},\mathbf{X}|\pi,\mu_{1},\mu_{2},\Sigma)=\prod_{n=1}^{N}[\pi N(x_{n}|\mu_{1},\Sigma)]^{t_{n}}[(1-\pi) N(x_{n}|\mu_{2},\Sigma)]^{1-t_{n}}\]
となる。まず\(\pi\)を求める。
\(\pi\)について対数尤度を、微分し0とおくと、
\[\pi=\frac{1}{N}\sum_{n=1}^{N}t_{n}=\frac{N_{1}}{N}\]
となる。
次に、\(\mu_{1}\)について考える。
さっきと同じ用に、\(\mu_{1}\)について対数尤度を、微分し0とおくと、
\[\mu_{1}=\frac{1}{N_{1}}\sum_{n=1}^{N}t_{n}x_{n}\]
となる。同様に、\(\mu_{2}\)についても、
\[\mu_{2}=\frac{1}{N_{2}}\sum_{n=1}^{N}t_{n}x_{n}\]
となる。
最後に\(\Sigma\)について考える。
対数尤度をとり、正規分布の標準的な最尤推定解をみると、
\[\Sigma=\frac{N_{1}}{N}S_{1}+\frac{N_{2}}{N}S_{2}\]
\[S_{1}=\frac{1}{N_{1}}\sum_{n\in C_{1}}(x_{n}-\mu_{1})(x_{n}-\mu_{1})^{\mathrm{T}}\]
\[S_{2}=\frac{1}{N_{2}}\sum_{n\in C_{2}}(x_{n}-\mu_{2})(x_{n}-\mu_{2})^{\mathrm{T}}\]
となる。
離散的な入力の場合について考える。
この時、条件付き確率が、ベルヌーイ分布の尤度関数と一致するので、それに置き換えて同じように計算する。
また、正規分布を仮定してやってきたが、一般的に、指数型分布族を仮定するなら同じ方法でできる。</p>

<h2>確率的識別モデル<a id="orgheadline40"></a></h2>

<p>事後事後分布\(p(C_{k}|x)\)のパラメータを反復再重み付け最小二乗(IRLS)を使い最尤推定し、直接求める。
ここでは、入力を基底関数\(\phi(x)\)を用いて非線形変換する。
2クラス分類について考える。
事後確率は、ロジスティクスシグモイド関数で表されたので、
\[p(C_{k}|x)=\sigma(w^{\mathrm{T}}\phi(x))=y(\phi)\]
となる。このモデルをロジスティクス回帰という。
尤度関数について考える。
教師データを\(\{\phi_{n},t_{n}\}\)とすると、尤度関数は、
\[p(\mathbf{t}|w)=\prod_{n=1}^{N}y_{n}^{t_{n}}\{1-y_{n}}^{1-t_{n}\}\]
となる。負の対数尤度を取ると、
\[E(W)=-\sum_{n=1}^{N}\{t_{n}\ln y_{n}+(1-t_{n})\ln(1-y_{n})\}\]
となる。これは交差エントロピー誤差関数と言われる。
パラメータの更新には、ニュートン-ラフソン法を使う。
このアルゴリズムでは、パラメータは、
\[w^{new}=w^{old}-H^{-1}\nabla E(w)\]
\[\nabla E(w)=\Phi^{\mathrm{T}}(\mathbf{y}-\mathbf{t})\]
\[H=\nabla\nabla E(w)=\Phi^{\mathrm{T}}\mathbf{R}\Phi\]
\[R_{nn}=y_{n}(1-y_{n})\]
で更新される。
式を整理すると、
\[w^{new}=(\Phi^{\mathrm{T}}\mathrm{R}\Phi)^{-1}\Phi^{\mathrm{T}}\mathbf{R}z\]
\[z=\Phi w^{old}-\mathbf{R}^{-1}(\mathbf{y}-\mathbf{t})\]
となる。\(\mathbf{R}\)がパラメータを含んでいるので、\(\mathbf{R}\)を先に決め、パラメータを更新するのを繰り返す。
多クラスについて考える。
事後確率は、ソフトマックス関数によって与えられたので、
\[p(C_{k}|x)=\frac{\exp(a_{k})}{\sum_{j}\exp(a_{j})}=y(\phi)\]
\[a_{k}=w_{k}^{\mathrm{T}}\phi\]
となる。
尤度関数は、
\[p(\mathbf{T}|w_{1},&hellip;,w_{k})=\prod_{n=1}^{N}\prod_{k=1}^{K}y_{nk}^{t_{nk}}\]
となる。負の対数尤度を取ると
\[E(w_{1},&hellip;,w_{k})=-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\ln y_{nk}\]
となり、交差エントロピー誤差関数となる。よってさっきと同じように、IRLSで最尤推定解がもとめられる。
これまで、事後確率をシグモイド関数で見てきたがすべての指数型分布族がシグモイド関数になるわけではない。
線形識別モデル
\[p(t=1|a)=f(a)\]
\[a=w^{\mathrm{T}}\phi\]
を考える。活性化関数には、しきい値以上ならば1、より小さいなら0を出力するものを考え、しきい値が\(p(\theta)\)から発生するとすると、
\[f(a)=\int_{-\infty}^{a}p(\theta)d\theta\]
となる。ここで、しきい値が平均0分散1の正規分布であると仮定すると、
\[\Phi(a)=\int_{-\infty}^{a}N(\theta|0,1)d\theta\]
となる。これは、プロビット関数の逆関数となる。また、一般的な正規分布でもパラメータが変わるだけなので、同じ形になる。
ここで、
\[erf(a)=\frac{2}{\sqrt{\pi}}\int_{0}^{a}\exp(-\theta^{2})d\theta\]
で表されるerf関数を使うとプロビット関数の逆関数は、
\[\Phi(a)=\frac{1}{2}\{1+erf(\frac{a}{\sqrt{2}})\}\]
となる。このように、プロビット関数での線形識別モデルをプロビット回帰という。パラメータは、ロジスティクス回帰と同じように求められる。</p>

<h2>ベイズロジスティクス回帰<a id="orgheadline43"></a></h2>

<p>ロジスティクス関数をラプラス近似し、正規分布で表現することで、ロジスティクス回帰をベイズ的に扱うことを考える。</p>

<h3>ラプラス近似<a id="orgheadline41"></a></h3>

<p>ある分布を正規分布で近似する方法。
まずは、連続な1変数zで考える。
近似したい分布を
\[p(x)=\frac{1}{z}f(z)\]
\[Z=\int f(z)dz\]
とする。ここでは、p(x)のモードを中心とした正規分布を考える。まずp(z)のモードを求める。モードは、
\[\frac{df(z)}{dz}|_{z=z_{0}}=0\]
を満たす\(z_{0}\)である。ここで、\(\ln f(z)\)の\(z_{0}\)におけるテイラー展開を考えると、
\[\ln f(z)\simeq\ln f(z_{0})-\frac{1}{2}A(z-z_{0})^{2}\]
\[A=-\frac{d^{2}}{dz^{2}}\ln f(z)|_{z=z_{0}}\]
となる。テイラー展開の指数を取ると、
\[f(z)\simeq f(z_{0})-\exp\{-\frac{A}{2}(z-z_{0})^{2}\}\]
となる。これをガウス分布で近似すると、
\[q(z)=(\frac{A}{2\pi})^{\frac{1}{2}}\exp\{-\frac{A}{2}(z-z_{0})^{2}\}\]
となる。
今度は、M次元ベクトル\(\mathbf{z}\)について考える。モードは、同じようにもとめられる。モードにおけるテイラー展開を考えると、
\[\ln f(z)\simeq\ln f(z_{0})-\frac{1}{2}(\mathbf{z}-\mathbf{z}_{0})^{\mathrm{T}}A(\mathbf{z}-\mathbf{z}_{0})\]
\[A=-\nabla\nabla\ln f(\mathbf{z})|_{\mathbf{z}=\mathbf{z}_{0}}\]
となる。両辺の指数を取り、正規分布で近似すると、
\[q(z)=N(\mathbf{z}|\mathbf{z}_{0},A^{-1})\]
となる。</p>

<h3>予測分布<a id="orgheadline42"></a></h3>

<p>ロジスティクス回帰の予測分布を考える。まず事後分布を考える。事後分布は正規分布に近似されているので、事前分布にも正規分布を使う。事前分布は、
\[p(w)=N(w|m_{0},S_{0})\]
となる。\(m_{0}、S_{0}\)はハイパーパラメータ。
ロジスティクス回帰の尤度関数を使って事後分布を表すと、
\[\ln p(w|\mathbf{t})=-\frac{1}{2}(w-m_{0})^{\mathrm{T}}S_{0}^{-1}(w-m_{0})+\sum_{n=1}^{N}\{t_{n}\ln y_{n}+(1-t_{n}) \ln(1-y_{n})+定数\}\]
となる。これをラプラス近似する。
モードは、MAP推定の解と一致する。ヘッセ行列は、
\[S_{N}^{-1}=S_{0}^{-1}+\sum_{n=1}^{N}y_{n}(1-y_{n})\phi_{n}\phi_{n}^{\mathrm{T}}\]
となる。よってラプラス近似は、
\[q(w)=N(w|w_{MAP},S_{N})\]
となる。予測分布について考える。
予測分布は、
\[p(C_{1}|\phi,\mathbf{t})=\int p(C_{1}|\phi,w)p(w|\mathbf{t})dw\simeq\int \sigma(w^{\mathrm{T}\phi})q(w)dw\]
となる。ここで、\(a=w^{\mathrm{T}}\phi\)とすると、
\[\sigma(w^{\mathrm{T}}\phi)=\int\delta(a-w^{\mathrm{T}}\phi)\sigma(a)da\]
となる。これより、
\[\int \sigma(w^{\mathrm{T}\phi})q(w)dw=\int \sigma(a)p(a)da\]
\[p(a)=\int\delta(a-w^{\mathrm{T}}\phi)q(w)dw\]
となる。このとき、q(w)が正規分布なので、p(a)も正規分布となる。
この時のp(a)の平均と分散は、
\[\mu_{a}=w_{MAP}^{mathrm{T}}\phi\]
\[\sigma^{2}_{a}=\phi^{\mathrm{T}}S_{N}\phi\]
となる。よって予測分布は、
\[p(C_{1}|\mathbf{t})=\int\sigma(a)N(a|\mu_{a},\sigma^{2}_{a})da\]
となる。ここで、プロビット関数の逆関数で、シグモイド関数を近似すると、
\[\sigma(a)\simeq\Phi(\lambda a)\]
となる。プロビット関数の逆関数と正規分布の畳み込みは、
\[\int \Phi(\lambda a)N(a|\mu,\sigma^{2})da=\Phi(\frac{\mu}{(\lambda^{-2}+\sigma^{2})^{\frac{1}{2}}})\]
となる。よって、予測分布は、
\[p(C_{1}|\phi,\mathbf{t})\simeq\sigma{k(\sigma^{2}_{a}\mu_{a})}\]
\[k(\sigma^{2})(1+\frac{\pi\sigma^{2}}{8})^{-\frac{1}{2}}\]
となる。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PRML3章]]></title>
    <link href="http://jinopapo.github.io/blog/2016/08/17/prml3/"/>
    <updated>2016-08-17T14:57:07+09:00</updated>
    <id>http://jinopapo.github.io/blog/2016/08/17/prml3</id>
    <content type="html"><![CDATA[<h1>線形回帰モデル<a id="orgheadline34"></a></h1>

<h2>線形基底関数モデル<a id="orgheadline29"></a></h2>

<p>任意の関数を基底関数として、重みパラメータに関する線形関数をつくり、入力xに対して目的変数tを推定する問題について考える。
最も簡単なモデルは、
\[y(x,w)=w_{0}+w_{1}x_{1}+&hellip;+w_{D}x_{D}\]
で表される。
これは、あまりに表現力がないので、拡張すると、
\[y(\mathbf{x},\mathbf{w})=w_{0}+\sum_{j=0}^{M-1}w_{j}\phi_{j}(\mathbf{x})\]
となる。\(\phi_{j}(\mathbf{x})\)を基底関数という。
また、\(w_{0}\)をバイアスパラメータという。
基底関数には、ガウス基底関数や、シグモイド基底関数、フーリエ基底関数などが用いられる。
最尤推定について考える。
推定したい関数yの出力にガウスノイズを乗せたものを目標変数にする。
\[t=y(x,w)+\epsilon\]
\(\epsilon\)はガウス分布に従うので、
\[p(t\mid x,w,\beta)=N(t\mid y(x,w),\beta^{-1})\]
で表せる。
誤差関数として、二乗損失関数を用いると最尤推定解は、条件付き期待値で与えられるので、
\[E[t\mid x]=\int tp(t\mid x)dt=y(x,w)\]
となる。
ここで、入力X={\(x_{1},&hellip;,x_{N}\)}と目的変数T={\(t_{1},&hellip;t_{N}\)}からなるデータ集合について考える。
二乗損失関数を用いて、対数尤度関数を考えると、
\[p(T\mid X,w,\beta)=\sum_{n=1}^{N}\ln N(t_{n}\mid w^{T}\phi(x_{n}),\beta^{-1})\
=\frac{N}{2}\ln\beta-\frac{N}{2}\ln(2\pi)-\beta E_{D}(W)\]
\[E_{D}(w)=\frac{1}{2}\sum_{n=1}^{N}\{t_{n}-w_{\mathrm{T}}\phi(x_{n})\}\]
となる。wの最尤推定解を求めると、
\[w_{ML}=(\Phi^{\mathrm{T}}\Phi)^{-1}\Phi^{\mathrm{T}}\mathbf{t}\]
となる。これは、最小二乗問題の正規方程式と言われる。\(\Phi\)は計画行列といわれ、その要素は、\(\Phi_{nj}=\phi_{j}(x_{n})\)であたえられる。
\[\Phi'\equiv(\Phi^{\mathrm{T}}\Phi)^{-1}\Phi^{\Phi^{\mathrm{T}}}\]
は、ムーア-アーペンローズの擬似逆行列と言われる。
\(\beta\)の最尤推定を求めると、
\[\frac{1}{\beta_{ML}}=\frac{1}{N}\sum_{n=1}^{N}\{t_{N}-w_{ML}^{\mathrm{T}\Phi(x_{n})}\}^{2}\]
となる。
逐次学習について考える。
確率的勾配降下法を適用して、逐次学習する。
誤差関数が、データ点の和からなっている場合、パラメータを
\[w^{\tau+1}=w^{\tau}-\eta\nabla E_{n}\]
を用いて更新していく。
今回の場合は、二乗誤差を更新していく。
また、過学習を防ぐために、正則化項を加えることを考えると誤差関数は、
\[E_{D}(w)+\lambda E_{W}(w)\]
で一般的に表せる。
最も単純な正則化項は
\[E_{W}(w)=\frac{1}{2}w^{\mathrm{T}}w\]
で与えられる。これは、逐次学習中に必要のない基底関数の重みが0に近づいていくので、荷重減衰といわれる。</p>

<h2>バイアスバリアンス分解<a id="orgheadline30"></a></h2>

<p>モデルの複雑差について考える。
線形基底関数モデルの期待二乗誤差は、
\[E[L]=\int \{y(x)-h(x)\}^{2}p(x)dx+\int \int \{h(x)-t\}^{2}p(x,t)dxdt\]
で表せる。h(x)は最適解を表す。
第二項は、ノイズを表すため、第一項を最小にするy(x)を求めるのが理想となる。
ここで、頻度主義での不確実性について考える。
頻度主義では、不確実性を複数のデータセットの平均で与える。
予測関数は、データセットごとで異なるので、y(x;D)で表す。
期待二乗誤差の第一項を各データ集合ごとの期待値\(E_{D}[y(x;D)]\)を用いて展開し、期待値を取ると、
\[E_{D}[\{y(x;D)-h(x)\}]=\{E_{D}[y(x;D)]-h(x)\}^{2}+E_{D}[\{y(x;D)-E_{D}[y(x;D)]\}^{2}]\]
となる。第一項はバイアスといわれ、理想値の離れ具合を示す。第二項は、バリアンスといわれ、データに対する敏感さを表す。
バリアンスとバイアスはトレードオフの関係にあり、適切に重みを選ぶ必要がある。
実際には、複数のデータセットが与えられることが少ないので、別のアプローチのほうがよい</p>

<h2>ベイズ線形回帰<a id="orgheadline31"></a></h2>

<p>ベイズ的な視点から、線形回帰モデルを扱う。
尤度関数\(P(t|w)\)がwのの二次関数の指数なので、事前分布は、
\[p(w)=N(w\mid m_{0},S_{0})\]
で表せる。
条件付きガウス分布の式より事後分布は、
\[p(w\mid t)=N(w\mid m_{N},S_{N})\]
\[m_{N}=S_{N}(S_{0}^{-1}m_{0}+\beta\Phi^{\mathrm{T}}t)\]
\[S_{N}^{-1}=S_{0}^{-1}+\beta\Phi^{\mathrm{T}}\Phi\]
となる。
簡単のため事前分布を、
\[p(w\mid \alpha)=N(w\mid 0,\alpha^{-1}I)\]
で表す。
ここでは、パラメータwではなく目的変数tがほしいので、予測分布について考える。
予測分布は、
\[p(t|\mathbf{t},\alpha,\beta)=\int p(t|w,\beta)p(w|\mathbf{t},\alpha,\beta)dw\]
で与えられる。
目的変数の条件付き分布、パラメータの事後分布は、ともに正規分布なので、
\[p(t|x,\mathbf{t},\alpha,\beta)=N(t|m_{N}^{T}\phi(x),\sigma^{2}_{N}(x))\]
\[\sigma_{N}^{2}=\frac{1}{\beta}+\phi(x)^{T}S_{N}\phi(x)\]
で表せる。
事前分布は、未知のパラメータによって、ガンマ分布や、ガウスーガンマ分布をつかいわける。
また、予測平均は、
\[y(x,m_{N})=\sum_{n=1}^{N}\beta\phi(x)^{T}S_{N}\phi(x_{n})t_{n}\]
で表わせ、目標変数tの線形結合として捉えれるので、
\[y(x,m_{N})=\sum_{n=1}^{N}k(x,x_{n})t_{n}\]
\[k(x,x^{\prime})=\beta\phi(x)^{\mathrm{T}}S_{N}\phi(x^{\prime})\]
と表せる。\(k(x,x^{\prime})\)を等価カーネルといい、目標変数の線形結合で、予測する回帰関数を線形平滑器という。
等価カーネルを用いると、各目標変数に重みを与えて、その線形結合で、新しいxに対する予測を出力することができる。</p>

<h2>ベイズモデル比較<a id="orgheadline32"></a></h2>

<p>ベイズの立場からのモデルの比較について考える。
頻度主義では、クロスバリデーションなどにでテスト用にデータをとって確認したが、ベイズでは一つのデータでできる。
L個のモデル\(M_{i}(i=1,&hellip;,L)\)について比較することを考える。
ベイズの定理を用いて、訓練集合Dが与えられた時の、モデルの事後分布
\[p(M_{i}|D)\propto p(M_{i})p(D|M_{i})\]
を評価する。この時の\(p(D|M_{i})\)は、モデルエビデンスといわれ、データから見たモデルの好みを表す。
二つのモデルのエビデンスの比を、ベイズ因子という。
モデルエビデンスは、
\[p(D|M_{i})=\int p(D|w,M_{i})p(w|M_{i})dw\]
で表わせ、尤度関数のパラメータwに関する周辺化として、捉えることもできる。
モデル選択により選ばれるモデルは、ベイズの観点においては、中間くらいの複雑さのモデルが選ばれる。</p>

<h2>エビデンス近似<a id="orgheadline33"></a></h2>

<p>事前分布のパラメータ\(\alpha\)、尤度関数のパラメータ\(\beta\)に関しても、データから推定することを考える。
ここでは、パラメータwに関して周辺化した周辺尤度を最大にするように、ハイパーパラメータ\(\alpha、\beta\)を決めるエビデンス近似について考える。
ハイパーパラメータに事前分布を追加すると、予測分布は、
\[p(t|\mathbf{t})=\int \int \int p(t|w,\beta)p(w|\mathbf{t},\alpha,\beta)p(\alpha,\beta|\mathbf{t})d\alpha d\beta dw\]
と表せる。入力xは省略。
事後分布\(p(\alpha,\beta|\mathbf{t})\)が\(\hat{\alpha}、\hat{\beta}\)で尖ってているとしたとき、予測分布は、
\[p(t|\mathbf{t},\hat{\alpha},\hat{\beta})=\int p(t|w,\hat{\beta})p(w|\mathbf{t},\hat{\alpha},\hat{\beta})dw\]
で表されるwの周辺化で近似できる。
ここで事後分布\(p(\alpha,\beta|\mathbf{t})\)について考える。
ベイズの定理より、
\[p(\alpha,\beta|\mathbf{t})\propto p(\mathbf{t}|\alpha,\beta)p(\alpha,\beta)\]
で事後分布が表される。
事前分布が比較的平坦なとき、尤度関数を最大にすることで、\(\hat{\alpha}\)と\(\hat{\beta}\)を得ることができる。
最大にするには、導関数を0とおき、方程式を解く方法とEMアルゴリズムを使う方法がある。
ここでは、方程式を解く方法について考える。
周辺尤度関数\(p(\mathbf{t}|\alpha,\beta)\)は、
\[p(\mathbf{t}|\alpha,\beta)=\int p(\mathbf{t}|w,\beta)p(w|\alpha)dw\]
で表すことができる。ここで、wの事前分布が\(N(w|0,\alpha^{-1})\)、尤度関数が、\(N(t|w^{\mathrm{T}}\phi(x),\beta^{-1})\)なので、
\[p(\mathbf{t}|\alpha,\beta)=(\frac{\beta}{2\pi})^{\frac{N}{2}}(\frac{\alpha}{2\pi})^{\frac{M}{2}}\int\exp\{-E(w)\}dw\]
\[E(w)=\frac{\beta}{2}||\mathbf{t}-\Phi w||^{2}+\frac{\alpha}{2}w^{T}w\]
と式変形ができる。
ここで、\(E(w)\)をwに関して平方完成すると、
\[E(w)=E(m_{n})+\frac{1}{2}(w-m_{N})^{\mathrm{T}}A(w-m_{N})\]
\[E(m_{N})=\frac{\beta}{2}||\mathbf{t}-\Phi m_{N}||^{2}+\frac{\alpha}{2}m_{N}^{T}m_{N}\]
\[A=\alpha I+\beta\Phi^{\mathrm{T}}\Phi\]
\[m_{N}=\beta A^{-1}\Phi^{T}\mathbf{t}\]
と表せる。このとき、\(m_{N}\)は事後分布の平均になる。
以上より対数尤度を取ると、
\[\ln p(\mathbf{t}|\alpha,\beta)=\frac{M}{2}\ln\alpha+\frac{N}{2}\ln\beta-E(m_{N})-\frac{1}{2}\ln|A|-\frac{N}{2}\ln(2\pi)\]
となる。
最初に、\(\alpha\)について考える。
まず、\(\ln|A|\)の微分について考える。
固有ベクトル方程式
\[(\beta\Phi^{\mathrm{T}}\Phi)u_{i}=\lambda_{i}u_{i}\]
より、\(A\)は固有値\(\alpha-\lambda_{i}\)をもつので、
\[\frac{d}{d\alpha}\ln|A|=\sum_{i}\frac{1}{\lambda_{i}+\alpha}\]
となる。
よって導関数を0とおくと、
\[0=\frac{M}{2\alpha}-\frac{1}{2}m_{N}^{\mathrm{T}}m_{N}-\frac{1}{2}\sum_{i}\frac{1}{\lambda_{i}+\alpha}\]
\[\alpha m_{N}^{\mathrm{T}}m_{N}=M-\alpha\sum_{i}\frac{1}{\lambda_{i}+\alpha}\]
\[\alpha m_{N}^{\mathrm{T}}m_{N}=\sum_{i}\frac{\lambda_{i}}{\lambda_{i}+\alpha}\]
となる。
この時の右辺を、\(\gamma\)とおくと、\(\alpha\)の最尤推定解は、
\[\alpha=\frac{\gamma}{m_{N}^{\mathrm{T}}m_{N}}\]
を満たすことがわかる。これは、最初に、\(\lambda\)と\(m_{N}\)を決め、\(\alpha\)を更新していき、収束するまで繰り返していくことで解ける。
次に\(\beta\)について考える。
\[\frac{d\lambda_{i}}{d\beta}=\frac{\lambda_{i}}{\beta}\]
より、
\[\frac{d}{d\beta}\ln|A|=\frac{\gamma}{\beta}\]
なので、導関数を0とおくと、
\[0=\frac{N}{2\beta}-\frac{1}{2}\sum_{n=1}^{N}\{t_{n}-m_{N}^{\mathrm{T}}\phi(x_{n})\}^{2}-\frac{\gamma}{2\beta}\]
\[\frac{1}{\beta}=\frac{1}{N-\gamma}\sum_{n=1}^{N}\{t_{n}-m_{N}^{\mathrm{T}}\phi(x_{n})\}^{2}\]
となる。これもさっきと同じように繰り返し計算して解ける。
また、この時、\(\gamma\)はデータによって影響を受けるパラメータ数、有効パラメータ数を示す。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PRML2章]]></title>
    <link href="http://jinopapo.github.io/blog/2016/08/15/prml2/"/>
    <updated>2016-08-15T16:20:44+09:00</updated>
    <id>http://jinopapo.github.io/blog/2016/08/15/prml2</id>
    <content type="html"><![CDATA[<h1>確率分布<a id="orgheadline28"></a></h1>

<h2>二値変数<a id="orgheadline12"></a></h2>

<h3>ベルヌーイ分布<a id="orgheadline8"></a></h3>

<p>コイン裏表のように、結果が二値しかない場合の分布を考える。
x=1となる確率を\(\mu\)を使って
\[p(x=1|\mu)=\mu\]
と表すと確率分布は、
\[Bern(x|\mu)=\mu^{x}(1-\mu)^{1-x}\]
で表される。
これをベルヌーイ分布という。
平均と分散は、
\[E[x]=\mu\]
\[var[x]=\mu(1-\mu)\]</p>

<h3>二項分布<a id="orgheadline9"></a></h3>

<p>データがN個のときx=1となる分布を考える。
この時の分布を二項分布という。
二項分布は、全体をNx=1がになった回数をmとすると、
\[Bin(m|N,\mu)=\frac{N!}{(N-m)!m!}\mu^{m}(1-\mu)^{N-m}\]
となる。
平均と分散は
\[E[m]=N\mu\]
\[var[m]=N\mu(1-\mu)\]</p>

<h3>ベータ分布<a id="orgheadline10"></a></h3>

<p>二値の分布の事前分布にはベータ分布が用いられることが多い。
\(\gamma\)関数をもちいて
\[Beta(\mu|a,b)=\frac{\gamma(a+b)}{\gamma(a)\gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\]
で表される。
平均と分散は、
\[E[\mu]=\frac{a}{a+b}\]
\[var[\mu]=\frac{ab}{(a+b)^{2}(a+b+1)}\]
となる。
a、bによって分布の方が代わるので、a、bをハイパーパラメータという。
事前分布に用いられる理由は、尤度関数が、\(\mu^{x}(1-\mu)^{1-x}\)に比例していて、事前分布にベータ分布を用いると事後分布も\(\mu^{x}(1-\mu)^{1-x}\)に比例するから。
これを性質を共役性という。</p>

<h3>比較<a id="orgheadline11"></a></h3>

<p>観測データDが得られたときの、頻度主義での分布モデルについて考える。
Dが独立して得られたとすると、尤度関数は
\[p(D|\mu)=\prod_{n=1}^{N}p(x_{n}|\mu)=\prod_{n=1}^{N}\mu^{x_{n}}(1-\mu)^{1-x_{n}}\]
で表わせる。これを対数を取り最尤推定すると、
\[\mu_{ML}=\frac{1}{N}\sum_{n=1}^{N}x_{n}\]
となる。
ベイズ主義からの分布モデルについて考える。
事後分布は、
\[p(\mu|m,l,a,b)=\frac{\gamma(m+a+l+b)}{\gamma(m+a)\gamma(l+b)}\mu^{m+a-1}(1-\mu)^{l+b-1}\]
となる。
また、この式から事後分布は事前分布からa、bをm、lだけ増やした形になっていて、データが増えた時に同じように分布が更新できることがわかる。
逐次学習中のある時間での分布は、
\[p(x=1|D)=\frac{m+a}{m+a+l+b}\]
となる。</p>

<h2>多値変数<a id="orgheadline15"></a></h2>

<h3>多項分布<a id="orgheadline13"></a></h3>

<p>多次元ベクトルで\(\sum_{k=1}^{K} x_{k}=1\)となるベクトルの分布を考える。
この時のxの分布は、
\[p(x|\mu)=\prod_{k=1}^{K}\mu_{k}^{x_{k}}\]
となり、ベルヌーイ分布を2種類以上の出力に一般化した物になる。
N個のデータ集合Dが得られたとすると尤度関数は、
\[p(D|\mu)=\prod_{n=1}^{N}\prod_{k=1}^{K}\mu_{k}^{x_{nk}}=\prod_{k=1}^{K}\mu_{k}^{\sum_{n}x_{nk}}=\prod_{k=1}^{K}\mu_{k}^{m_{k}}\]
となる。
よって、\(m_{k}=\sum_{n}x_{nk}\)に依存していることがわかる。
したがって、m<sub>k</sub>の同時確率分布について考えると
\[Mult(m_{1}&hellip;m_{k}|\mu,N=\frac{N!}{m_{1}!&hellip;m_{k}!})\prod_{k=1}^{K}mu_{k}^{m_{k}}\]
となる。
これは多項分布と呼ばれる。
これの最尤推定はラグランジュ乗数(\lambda)を用いて
\[\sum_{k=1}^{K}m_{k}ln\mu_{k}+\lambda(\sum_{k=1}^{K}\mu_{k}-1)\]
を最大化すると
\[\mu_{k}^{ML}=\frac{m_{k}}{N}\]</p>

<h3>ディリクレ分布<a id="orgheadline14"></a></h3>

<p>多項分布の共役分布を正規化すると、
\[Dir(\mu|\alpha)=\frac{\gamma(\alpha_{0})}{\gamma(\alpha_{1}..\gamma(\alpha_{k}))}\prod_{k=1}^{K}\mu_{k}^{\alpha_{k}-1}\]
となりディリクレ分布と呼ばれる。</p>

<h2>ガウス分布<a id="orgheadline23"></a></h2>

<p>よく使われる分布。
一変数の場合
\[N(x|\mu,\sigma^{2})\frac{1}{(2\pi\sigma^{2})^\frac{1}{2}}exp{-\frac{1}{2\sigma^{2}}(x-\mu)^{2}}\]
D次元ベクトルの場合
\[N(x|\mu,\sigma^{2})\frac{1}{(2\pi)^\frac{D}{2}}exp{-\frac{1}{2}(x-\mu)^{T}\sigma^{-1}(x-\mu)}\]
となる。
基本的には一つのピークを持つ分布なので、近似しにくいため、潜在変数や、非観測変数などを使っていろいろ近似する。</p>

<h3>条件付きガウス分布と周辺ガウス分布<a id="orgheadline16"></a></h3>

<p>ある二つの変数集合がガウス分布に従うなら、条件付き分布と、周辺分布も正規分布になる。
\(\Lambda=\sigma^{-1}\)とした同時確率正規分布\(N(x|\mu,\sigma)\)があるとし、
\[x=(x_{a},x_{b}) \]
\[\mu=(\mu_{a},\mu_{b})\]
\[\sigma=\left(
\begin{array}{cc}
\sigma_{aa} &amp; \sigma_{ab} \\
\sigma_{ba} &amp; \sigma_{bb}
\end{array}
\right)\]
\[\Lambda=\left(
\begin{array}{cc}
\Lambda_{aa} &amp; \Lambda_{ab} \\
\Lambda_{ba} &amp; \Lambda_{bb}
\end{array}
\right)\]
で分割する。
条件付きガウス分布は、
\[p(x_{a}|x_{b})=N(x_{a}|\mu_{a|b},\Lambda_{aa}^{-1})\]
\[\mu_{a|b}=\mu_{a}-\Lambda_{aa}^{-1}\Lambda_{ab}(x_{b}-\mu_{b})\]
この時の平均はx<sub>b</sub>に関する線形の式とみれるので、線形ガウス分布と言われる。
周辺ガウス分布は、
\[p(x_{a})=N(x_{a}|\mu_{a},\sigma_{aa})\]
で表される。
また周辺分布と条件付き分布が
\[p(x)=N(x|\mu,\Lambda^{-1})\]
\[p(y|x)=N(y|Ax+b,L^{-1})\]
で与えられたとき、ベイズの定理を用いるとyの周辺分布、条件付き分布は、
\[p(y)=N(y|A\mu+b,J^{-1}+A\Lambda^{-1}A^{T})\]
\[p(x|y)=N(x|\sigma{A^{T}L(y-b)+\Lambda\mu},\sigma)\]
\[\sigma=(\Lambda+A^{T}LA)^{-1}\]</p>

<h3>Robbins-Monroアルゴリズム<a id="orgheadline17"></a></h3>

<p>同時確率分布\(p(z,\theta)\)があるとき、zの条件付き確率を\(f(\theta)\)として定義する。
この関数を回帰関数という。
Robbins-Monroアルゴリズムでは、回帰関数の\(f(\theta^{*})=0\)となる点を探すことが目的となる。
\(\theta\)の逐次的な推定は、
\[\theta^{(N)}=\theta^{(N-1)}-a_{N-1}z(\theta^{(N-1)})\]
となる。
この時\(a_{n}\)は、
\[\lim_{Z \to \infty}a_{N}=0\]
\[\sum_{N=1}^{\infty}a_{N}=\infty\]
\[\sum_{N=1}^{\infty}a_{N}^{2}&lt;\infty\]
を満たす整数の系列でなければならない。</p>

<h3>ガンマ分布<a id="orgheadline18"></a></h3>

<p>\[Gam(\lambda|a,b)=\frac{1}{\Gamma(a)}b^{a}\lambda^{a-1}exp(-b\lambda)\]
で定義される分布。
平均と分散は、
\[E[\lambda]=\frac{a}{b}\]
\[var[\lambda]=\frac{a}{b^{2}}\]</p>

<h3>スチューデントのt分布<a id="orgheadline19"></a></h3>

<p>\begin{equation}
St(x\mid \mu,\lambda,\nu)=\frac{\Gamma(\frac{\nu}{2}+\frac{1}{2})}{\Gamma(\frac{\nu}{2})}(\frac{\lambda}{\pi\nu})^{\frac{1}{2}}[1+\frac{\lambda(x-\mu)^{2}}{\nu}]^{-\frac{\nu}{2}-\frac{1}{2}}
\end{equation}</p>

<p>で表される分布をt分布という。
\(\lambda\)は精度を表すが分散の逆数ではない。\(\nu\)は自由度。
一般的に正規分布よりもすそが広くロバスト性があるため、ノイズにつよい。
また最尤推定解は、EMアルゴリズムで求める。</p>

<h3>フォン・ミーゼス分布<a id="orgheadline20"></a></h3>

<p>ガウス分布で、周期的な変数を扱いたい時に使える分布。例えば、風向きの予測など。
二変数の正規分布は、
\[p(x_{1},x_{2})=\frac{1}{2\pi\sigma^{2}}\exp\{-\frac{(x_{1}-\mu_{1})^{2}+(x_{2}-\mu_{2}^{2})}{2\sigma^{2}}\}\]
で表せる。
極座標系で表現しなおし正規化すると、
\[p(\theta\mid\theta_{0},m)=\frac{1}{2\pi I_{0}(m)}\exp\{m\cos(\theta-\theta_{0})\}\]
\[I_{0}(m)=\frac{1}{2\pi}\int_{0}^{2\pi}\exp\{m\cos\theta d\theta\}\]
\[m=\frac{r_{0}}{\sigma^{2}}\]
r<sub>0</sub>、&theta;<sub>0</sub>は平均に変数の極座標系でのパラメータ、&theta;<sub>0</sub>は平均、mは集中パラメータで、精度パラメータと同じ、\(I_{0}(m)\)は正規化係数。
最尤推定解について考える。
対数尤度関数は、
\[\ln p(D\mid\theta_{0},m)=-N\ln(2\pi)-N\ln(I_{0}(m))+m\sum_{n=1}^{N}\cos(\theta_{n}-\theta_{0})\]
となり導関数を0として解くと、
\[\theta_{0}^{ML}=\tan^{-1}{\frac{\sum_{n}\sin\theta_{n}}{\sum_{n}\cos\theta_{n}}}\]
となる。
mに関しては、
\[A(m_{ML})=\frac{1}{N}\sum_{n=1}{N}\cos(\theta_{n}-\theta_{0}^{ML})\]
となる。</p>

<h3>頻度主義とベイズ主義の比較<a id="orgheadline21"></a></h3>

<p>最尤推定によって逐次学習を行う場合を考える。
正規分布から、データ点が得られたとすると対数尤度は
\[ln{p(X\mid &mu;,&sigma;)}=-\frac{ND}{2}ln{2\pi}-\frac{N}{2}ln|&sigma;|-\frac{1}{2}&sum;_{n=1}^{N}(x_{n}-&mu;)^{\mathrm{T}}&sigma;^{-1}(x_{n}-&mu;)\]
となる。
\(\mu\)について微分した導関数を0とおくと
\[&mu;_{ML}=\frac{1}{N}&sum;_{n=1}^{N}x_{n}\]
となる。
\(\sigma\)については複雑だが、計算すると
\[&sigma;=\frac{1}{N}&sum;_{n=1}^{N}(x_{n}-&mu;_{ML})(x_{n}-&mu;_{ML})^{\mathrm{T}}\]
になる。
この時の\(\sigma\)にはバイアスがかかっていて過小評価されているが、
\[&sigma;=\frac{1}{N-1}&sum;_{n=1}^{N}(x_{n}-&mu;_{ML})(x_{n}-&mu;_{ML})^{\mathrm{T}}\]
にすると真に近づく。
逐次学習についてかんがえるため、\(x_{n}が&mu;_{ML}\)に与える影響について考えると、
\[&mu;_{ML}^{(N)}=&mu;_{ML}^{(N-1)}+\frac{1}{N}(x_{N}-&mu;_{ML}^{(N-1)})\]
となり、逐次学習の定式化ができた。
しかし、いつもこれでできるわけではないので、Robbins-Monroアルゴリズムを適用する。
この時、根は最尤推定の解に相当し、zは観測データになる。
\(a_{N}=\frac{&sigma;^{2}}{N}\)とすると、ガウスの最尤推定の式と一致する。
ベイズ主義によって逐次学習を行う場合を考える。
未知の情報が何かによって事前分布の選び方が変わってくる。
平均が未知の場合の尤度は、\(\mu\)の関数となり
\[p(X\mid &mu;)=\frac{1}{(2\pi&sigma;^{2})^{\frac{N}{2}}}exp{-\frac{1}{2&sigma;^{2}}&sum;_{n=1}^{N}(x_{n}-&mu;)^{2}}\]
となり、事前分布には共役事前分布のガウス分布\(p(&mu;)=N(&mu;\mid &mu;_{0},&sigma;_{0}^{2})\)を選べばいいと解る。
この時、事後分布は、</p>

<p>\begin{equation}
p(\mu\mid X)=N(\mu\mid \mu_{N},\sigma_{N}^{2})
\end{equation}</p>

<p>\begin{equation}
\mu_{N}=\frac{\sigma^{2}}{N\sigma_{0}^{2}+\sigma^{2}}\mu_{0}+\frac{N\sigma_{0}^{2}}{N\sigma_{0}^{2}+\sigma^{2}}\mu_{ML}
\end{equation}</p>

<p>\begin{equation}
\frac{1}{\sigma_{N}^{2}}=\frac{1}{\sigma_{0}^{2}}+\frac{N}{\sigma^{2}}
\end{equation}</p>

<p>となる。
逐次学習については、尤度関数を事後分布に掛けあわせていくだけでよい・
分散が未知の場合の尤度関数は、精度\(\lambda\)を用いて</p>

<p>\begin{equation}
p(X|\lambda)\propto\lambda^{\frac{N}{2}}\exp\{-\frac{\lambda}{2}\sum^{N}_{n=1}(x_{n}-\mu)^{2}\}
\end{equation}</p>

<p>となり事前分布は、ガンマ分布\(Gam(&lambda;\mid a_{0},b_{0})\)を用いればいいとわかる。
事後分布は、</p>

<p>\begin{eqnarray}
p(\lambda\mid X)\propto Gam(\lambda\mid a_{N},b_{N}) \\
a_{N}=a_{0}+\frac{N}{2} \\
b_{N}=b_{0}+\frac{N}{2}\sigma^{2}_{ML}
\end{eqnarray}</p>

<p>となる。
平均と分散が未知の場合は、同時確率分布は</p>

<p>\begin{equation}
p(\mu,\lambda)\propto\exp\{-\frac{\beta\lambda}{2}(\mu-\frac{c}{\beta})^{2}\}\lambda^{\frac{\beta}{2}}\exp\{-(d-\frac{c^{2}}{2\beta})\lambda\}
\end{equation}</p>

<p>となる。c、d、\(\beta\)は定数である。
事前分布は、\(a=\frac{(1+\beta)}{2}、b=d-\frac{c^{2}}{2\beta}\)とすると</p>

<p>\begin{equation}
p(\mu,\lambda)=N(\mu\mid \mu_{0},(\beta\lambda)^{-1})Gam(\lambda\mid a.b)
\end{equation}</p>

<p>となる。
この分布は正規-ガンマ分布という。
D次元変数の場合についてかんがえる。
平均が未知の時は、事前分布はガウス分布になる。
分散が未知の場合は、事前分布は</p>

<p>\begin{eqnarray}
W(\Lambda\mid W,\nu)=B|\Lambda|^{\frac{(\nu-D-1)}{2}}\exp((-\frac{1}{2}\mathrm{Tr}(W^{-1}\Lambda))) \
B(W,\nu)=|W|^{\frac{\nu}{2}}(2^{\nu\frac{D}{2}}\pi^{D\frac{(D-1)}{4}}\prod_{i=1}^{D}\Gamma(\frac{\nu+1-i}{2}))^{-1}
\end{eqnarray}</p>

<p>となる。
これはウィシャート分布と言われ、\nuは、自由度パラメータ、Wは尺度行列、Bは正則化定数
分布と平均が未知の場合は、</p>

<p>\begin{equation}
p(\mu,\Lambda\mid \mu_{0},\beta,W,\nu)=N(\mu\mid \mu_{0},(\beta\Lambda)^{-1})W(\lambda\mid W,\nu)
\end{equation}</p>

<p>となり、ガウス-ウィシャート分布といわれる。</p>

<h3>混合ガウス分布<a id="orgheadline22"></a></h3>

<p>複雑なデータに関して一つのガウス分布だけでは、うまく近似できないことがある
複数個のガウス分布を重ねて混合ガウス分布にするとうまく近似できる
混合ガウス分布は
\[p(x)=\sum_{k=1}^{K}\pi_{k}N(x\mid\mu_{k},\sigma_{k})\]
\[\sum_{k=1}^{K}\pi_{k}\]
\[0\leq\pi_{k}\leq1\]
で表される。
\(N(x\mid\mu_{k},\sigma_{k})\)を混合要素、\(\pi_{k}\)を混合係数という。
また、混合係数を事前分布、混合要素を条件付き分布としてみると
\[p(x)=\sum_{k=1}^{K}p(k)p(x\mid k)\]
と表せて、これを負荷率という。
混合ガウス分布のパラメータの決定する方法について考える。
最尤推定で求めるとして、対数尤度関数は、
\[\ln p(X\mid \pi,\mu,\sigma)=\sum_{n=1}^{N}\ln{\sum_{k=1}^{K}\pi_{k}N(x_{n}\mid \mu_{k},\sigma_{k})}\]
となる。これを解くためには、EMアルゴリズムなどを使う。</p>

<h2>指数型分布族<a id="orgheadline24"></a></h2>

<p>\[p(x\mid \eta)=h(x)g(\eta)\exp\{\eta^{T}u(x)\}\]
で表されるのを指数型分布属という。
指数型分布族の尤度関数を求めるのに、必要なデータの値を十分統計量という。
指数型分布族では、一般に共役事前分布が存在する。
事前分布の一つとして、事後分布のあまり影響を与えないようにする無情報事前分布というものがある。
無情報事前分布の例として、平行移動不変性と、尺度不変性を持つ分布が挙げられる。
平行移動不変性を持つ分布は、
\[p(x\mid \mu)=f(x-\mu)\]
で表され、\(\mu\)を位置パラメータという。この時、分布は定数となる
尺度不変性を持つ分布は、
\[p(x\mid \sigma)=\frac{1}{\sigma}f(\frac{x}{\sigma})\]
で表され、\(\sigma\)は尺度パラメータという。この時、分布は、正規化できないので、変速事前分布と言われる。
変速事前分布をとった場合でも、事後分布が正規化できる場合は使われる</p>

<h2>ノンパラメトリック法<a id="orgheadline27"></a></h2>

<p>事前分布などを用いたパラメータを用いる方法ではなく、データ点から分布モデルをつくることについて考える。
D次元の分布p(x)からなるデータ点をもとに、p(x)を推定する。
小さな領域Rごとに密度が一定とみなせるほど、Rが小さく、平均が真に近くなるほどRにデータ点が含まれると仮定すると、
\[p(x)=\frac{K}{NV}\]
で近似できる。Kは領域に含まれるデータ点の数、Nはデータ点の総数、Vは領域の体積。</p>

<h3>カーネル密度推定<a id="orgheadline25"></a></h3>

<p>Vを固定し、領域内に含まれうKによってp(x)を推定する。
\[k(u)\geq0\]
\[\int k(u)du=1\]
を満たす任意の関数k(u)をカーネル関数として利用する。
Kは、
\[K=\sum_{n=1}^{N}k(\frac{x-x_{n}}{h})\]
により推定される。
このhがカーネル関数によって変換された空間の広さに値し、hを適切に選ぶとうまく近似できる。一般にカーネル関数には、ガウス関数が用いられる。</p>

<h3>最近傍法<a id="orgheadline26"></a></h3>

<p>Kを固定し、K個データ点が入るように、Vを決めp(x)を推定する。
いわゆるk近傍法。クラスタリングとかに使われる。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prml1]]></title>
    <link href="http://jinopapo.github.io/blog/2016/08/14/prml1/"/>
    <updated>2016-08-14T16:56:46+09:00</updated>
    <id>http://jinopapo.github.io/blog/2016/08/14/prml1</id>
    <content type="html"><![CDATA[<h1>序論<a id="orgheadline7"></a></h1>

<h2>回帰の例<a id="orgheadline1"></a></h2>

<p>機械学習でできることの一つの回帰を例に雰囲気を説明する。
wに関する線形の式\(y(x)=\sum^{M}_{j=0}w_{j}x^{j}\)でsin関数を近似する。
この時のMはモデルの次数、wは学習によって調節されるパラメータを表す。
学習用データには、実際の問題と同じようにノイズを乗せたsin関数の値を用いる。
誤差関数には二法誤差を使う。
学習によってこの誤差関数を最小にする用にパラメータを調節していく。
この時モデルの次数はとても大切で、適切な次数でないと、表現力が足りなかったり、過学習してしまう。
ベイズの用に確率を使うと、モデルの複雑さには依存しなくなる。
この過学習を防ぐ方法としては、正則化がよく用いられる。
正則化とは、誤差関数にペナルティを課して、パラメータが大きくなり過ぎない用にすること。
これによりノイズに強くなったりする。
モデルの複雑さを決める方法としては、クロスバリデーションとかある。</p>

<h2>確率論<a id="orgheadline5"></a></h2>

<h3>ベイズ　<a id="orgheadline2"></a></h3>

<p>ランダムな繰り返し試行の頻度を確率とするのを、頻度主義また古典確率という。
ランダムな繰り返し試行の不確かさを確率とするのをベイズ主義という。
ベイズでは、自分たちパラメータw対するの知見を、事前確率p(w)という形で確率に組み込み、観測されたデータDはp(D|w)という形で表される。
頻度主義では、データを元に、パラメータを更新していくのに対し、ベイズでは、wの不確かさが更新されて行く。
これらを元にベイズの定理を適用すると、
\[p(w|D)=\frac{p(D|w)p(w)}{p(D)}\]
となり、wのそれっぽさが確率で出せる。
また、p(D|w)は、wに関する関数ともとれ、尤度関数という。
p(D)は
\[p(D)=\int p(D|w)p(w)dw\]
に変換でき、p(w|D)はwの式とみなせる。</p>

<h3>正規分布<a id="orgheadline3"></a></h3>

<p>事前分布としてよく用いられる正規分布について触れる。
正規分布は
\[N(x|\mu,\sigma)=\frac{1}{(2\pi\sigma^{2})^{\frac{1}{2}}}exp{-\frac{1}{2\sigma^{2}(x-\mu)^{2}}}\]
で表される。
\(\mu\)を平均\(\sigma^{2}\)を分散とする。
データ集合Xが与えられたときのパラメータの決め方について考える。
\(\mu\)と\(\sigma\)が与えられたときのデータ集合の発生確率は、
\[p(X|\mu,\sigma^{2})=\prod^{N}_{n=1}N(X_{n}|\mu,\sigma^{2})\]
となり、これが尤度関数になる。
対数尤度を取り、最小化する\(\mu_{ML}\)、\(\sigma^{2}_{ML}\)を求めると
\[\mu_{ML}=\frac{1}{N}\sum^{N}{n=1}x_{n}\]
\[\sigma^{2}_{ML}=\frac{1}{N}\sum^{N}{n=1}(x_{n}-\mu_{ML})^{2}\]
となり、それぞれサンプル平均、サンプル分散と言われる。
この時、サンプル平均は、すぐ真に近づくが、サンプル分散はなかなか近づかない。
これをバイアスという。</p>

<h3>回帰の例<a id="orgheadline4"></a></h3>

<p>確率から回帰の例をもう一度見る。
入力xに対して求めたい値tの発生確率が正規分布に従うとすると
\[p(t|x,w,\beta)=N(t|y(x,w),\beta^{-1})\]
となる。
訓練データ{X,T}を用いると
\[p(T|X,w,\beta)=\prod_{n=1}^{N}N(T_{n}|y(X_{n},w),\beta^{-1})\]
で尤度関数が定義でき、最尤推定によりモデルのパラメータを決めることができる。
これは、二乗誤差でパラメータを最適化すると同じ。
ベイズ的な視点から回帰の例をみる。
事前確率を
\[p(w|\alpha)=N(w|0,\alpha^{-1}I)\]
とするとwの事後確率は、
\[p(w|X,T,\alpha,\beta)\propto p(T|X,w,\beta)p(w|\alpha)\]
となる。
与えられたデータから、事後確率を最大にするwを求めるのをMAP推定という。
これは、正則化を用いて二乗誤差でwを最適化するのと同じ。
完全なベイズでは、予測分布を作るので
\[p(t|x,X,T)=\int p(t|x,w,\beta)p(w|X,T,\alpha,\beta)dw\]
となる　</p>

<h2>決定論<a id="orgheadline6"></a></h2>

<p>入力に対して出力を一意に決める問題。
クラスタリングの例をみる。
入力xをクラスC<sub>k</sub>に当てはめる。
アプローチとしては、<br/>
( a )p(x|C<sub>k</sub>)を求めてベイズの定理を使いp(C_{}k}|x)を求める。これは、同時確率分布を求められ、出力も入力も人工的に生成できるので生成モデルと言われる<br/>
( b )p(C<sub>k</sub>)を直接求める。識別モデルと言われる<br/>
( c )クラスC<sub>k</sub>を出力する関数を求める<br/>
の三種類がある。
( a )( b )のアプローチでは、クラスを決定するしきい値を求める問題がある。
最も簡単な誤識別を最小にする考え方では、単純にもっとも確率の高い物を選べばいい。
複雑な物を例として、誤認識したときの損失を最小にする考え方では、認識ごとに重みをつけそれを最小化するものを選ぶ。
別の方法としては、分からないものとして棄却する方法もある。あるしきい値をきめて、各クラスになる確率がしきい値以下なら分からないものとする。
( c )のアプローチでは、しきい値も全部決めてしまうので、何か変更があった時とか一から学習しなおしたりするので、汎用性にかける。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[window8.1でxubuntuをデュアルブート]]></title>
    <link href="http://jinopapo.github.io/blog/2015/12/03/window8-dot-1dexubuntuwodeyuarubuto/"/>
    <updated>2015-12-03T00:56:30+09:00</updated>
    <id>http://jinopapo.github.io/blog/2015/12/03/window8-dot-1dexubuntuwodeyuarubuto</id>
    <content type="html"><![CDATA[<p>windows8.1でxubuntuをデュアルブートする時にハマった。</p>

<p>自分の場合はグラボ側にメインのモニターを繋いでいて、当然xubuntuにはGPUのドライバーが入っていないので、ちょっとこっとエラーのメッセージが出てその後、何も出力されなくなるってことになった。</p>

<p>最初の数時間はエラーメッセージに気が付かなくて、いろいろいじりまわしてたけどマザボ側の出力に変えるだけで解決。</p>

<p>ようやくxubuntuが起動し、インストール。これはすんなりいった。</p>

<p>その後、なんやかんやGPUのドライバーを入れwindowの起動を確認し、その日は終了。</p>

<p>次の日立ち上げるとwindowしかた立ち上がらず,xubuntuはHDD上には存在しているが起動するデバイスが存在しないと言われ立ち上がらず。</p>

<p>調べてみると、windpow8は、windowを起動するたびにwindowのブートローダーを最優先にするらしく一回windowsを起動してしまうと、grubの優先度が下がり見えなくなってしまいubuntuが起動できなくなる仕様らしい。</p>

<p>これを解決するにはwindowsのブートローダーを書き換えてubuntuをwindowsのブートローダーから立ち上がるようにするか、windowのブートローダーをgrubで上書きするのがいいらしい。</p>
]]></content>
  </entry>
  
</feed>
